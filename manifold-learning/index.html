<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Mark  Crowley | Manifold Learning</title>
<meta name="description" content="">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/manifold-learning/">

<!-- Theming-->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Mark</span>   Crowley
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/bio/">
                bio
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/contact/">
                contact
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/lab/">
                lab
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/news/">
                news
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/showcase/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/topics/">
                topics
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
      
          
            <h1 class="post-title">Manifold Learning</h1>
        
      
    <p class="post-description" style="border-bottom-style:dashed; border-bottom-color:lightgrey; border-bottom-width:1px; margin:1px; margin-bottom:10px;font-style: italic;">Manifold learning looks at ways to automatically extract meaningful features, dimensions or subspaces from data in order to build better models, expand data, reduce data, etc.</p>

      
      
      
      
      
      
            <p class="post-description" style="border-bottom-style:dashed; border-bottom-color:lightgrey; border-bottom-width:1px; margin:1px; margin-bottom:3px;">
            <b>WEBPAGE: </b> 
            
                <a href="https://www.amazon.ca/Elements-Dimensionality-Reduction-Manifold-Learning/dp/3031106016/ref=sr_1_1?keywords=9783031106019&linkCode=qs&qid=1659572815&returnFromLogin=1&s=books&sr=1-1">Elements of Dimensionality Reduction and Manifold Learning (Amazon)</a> 
            
            </p>
        
        <p></p>
  </header>

  <article>
      
      <img src="/assets/img/manifold-learning-book.jpg" style="width: 300px; padding: 10px; float: right;">
      
    <p>Manifold Learning and Dimensionality Reduction are vast areas of study in Math and Computer Science. The task is to find ways to determine the essential relationships and structure of a dataset. Researchers in this area looks at ways to automatically extract meaningful features, dimensions or subspaces from data in order to build better models, expand data, reduce data, etc.</p>

<p>The recent focus on <strong>Deep Learning</strong> seems to raise the question whether
dedicated research on <strong><a href="manifold-learning.md">Manifold Learning and Dimensionality Reduction</a></strong> are still required as their own pursuit since. After all, some form of Encoder-Decoder neural network could always be devised as a replacement.
While such systems work well given the right training process and enough data, there is also certainly a role to
be played by interpretable models built on solid statistical concepts.</p>

<p>Extraction of lower-dimensional representations of data can allow more compact storage or transmission and
also improve the performance of other ML tasks such as classification and regres_sion, as the more compact representation
must necessarily encode the most important relationships to maintain accuracy.</p>

<p>We have an exciting group of work which has been published in recent years on this topic which you can see below in the Publications list.</p>

<h3 id="upcoming-textbook-on-manifold-learning">Upcoming Textbook on Manifold Learning!</h3>

<p>This work has culiminated recently in the graduation of my first Doctoral student, <a href="">Benyamin Ghojogh</a>, in April 2021 with his thesis encompassing many of these advances.
Dr. Ghojogh continued as a postdoc in my lab until 2022 and now works in industry. In late 2022 we will publish, via Springer, a new textbook on <strong>“Manifold Learning and Dimensionality Reduction”</strong> <a class="citation" href="#ghojogh2022springerbook">(Ghojogh et al., 2023)</a> written in collaboration with Prof. Ali Godsi andd Prof. Fakhri Karray.</p>


  </article>


      
          
            <div class="publications">
              <h2>Our Papers on Manifold Learning</h2> 
            <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Textbook</abbr>
    
  
  </div>

  <div id="ghojogh2022springerbook" class="col-sm-8">
      <div class="title">
          
          Elements of Dimensionality Reduction and Manifold Learning
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://www.researchgate.net/profile/Benyamin-Ghojogh" target="_blank">Benyamin Ghojogh</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    <em>Mark Crowley</em>,
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://uwaterloo.ca/scholar/karray" target="_blank">Fakhri Karray</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      and <a href="https://uwaterloo.ca/data-analytics/" target="_blank">Ali Ghodsi</a>.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      

      
          Springer Nature,
      
      
      
          Feb,
      
      
        2023.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
      <a href="https://link.springer.com/book/10.1007/978-3-031-10602-6" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         Dimensionality reduction, also known as manifold learning, is an area of machine learning used for extracting informative features from data, for better representation of data or separation between classes. This book presents a cohesive review of linear and nonlinear dimensionality reduction and manifold learning. Three main aspects of dimensionality reduction are covered – spectral dimensionality reduction, probabilistic dimensionality reduction, and neural network-based dimensionality reduction, which have geometric, probabilistic, and information-theoretic points of view to dimensionality reduction, respectively. This book delves into basic concepts and recent developments in the field of dimensionality reduction and manifold learning, providing the reader with a comprehensive understanding. The necessary background and preliminaries, on linear algebra, optimization, and kernels, are also explained in the book to ensure a comprehensive understanding of the algorithms.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ghojogh2022canai" class="col-sm-8">
      <div class="title">
          
          Theoretical Connection between Locally Linear Embedding, Factor Analysis, and Probabilistic PCA
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://www.researchgate.net/profile/Benyamin-Ghojogh" target="_blank">Benyamin Ghojogh</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://uwaterloo.ca/data-analytics/" target="_blank">Ali Ghodsi</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://uwaterloo.ca/scholar/karray" target="_blank">Fakhri Karray</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>Canadian Conference on Artificial Intelligence</em>. 

      

      
          Canadian Conference on Artificial Intelligence (CAIAC),
      
      
          Toronto, Ontario, Canada.
      
      
          May,
      
      
        2022.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2022-canai-ghojogh-theoretical.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
      
      <a href="/assets/pdf/2022-canai-ghojogh-theoretical1.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
      <a href="https://caiac.pubpub.org/pub/7eqtuyyc" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    
      <a href="https://hyp.is/go?url=https%3A%2F%2Fassets.pubpub.org%2Fzbfq7fzb%2F71652816875953.pdf&group=__world__" class="btn btn-sm z-depth-0" role="button" target="_blank">Hypoth</a>
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         Locally Linear Embedding (LLE) is a nonlinear spectral dimensionality reduction and manifold learning method. It has two main steps which are linear reconstruction and linear embedding of points in the input space and embedding space, respectively. In this work, we look at the linear reconstruction step from a stochastic perspective where it is assumed that every data point is conditioned on its linear reconstruction weights as latent factors. The stochastic linear reconstruction of LLE is solved using expectation maximization. We show that there is a theoretical connection between three fundamental dimensionality reduction methods, i.e., LLE, factor analysis, and probabilistic Principal Component Analysis (PCA). The stochastic linear reconstruction of LLE is formulated similar to the factor analysis and probabilistic PCA. It is also explained why factor analysis and probabilistic PCA are linear and LLE is a nonlinear method. This work combines and makes a bridge between two broad approaches of dimensionality reduction, i.e., the spectral and probabilistic algorithms.	</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">QQE</abbr>
    
  
  </div>

  <div id="ghojogh2021mlwajrnl" class="col-sm-8">
      <div class="title">
          
          Quantile–Quantile Embedding for distribution transformation and manifold embedding with ability to choose the embedding distribution
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://www.researchgate.net/profile/Benyamin-Ghojogh" target="_blank">Benyamin Ghojogh</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://uwaterloo.ca/scholar/karray" target="_blank">Fakhri Karray</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
        <em>Machine Learning with Applications (MLWA)</em>.

          
              6,
          
          

      

      
      
      
      
        2021.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2006.11385" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
      
      <a href="/assets/pdf/2021-mlwajrnl-ghojogh-quantile%E2%80%93quantile.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://doi.org/10.1016/j.mlwa.2021.100088" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         We propose a new embedding method, named Quantile-Quantile Embedding (QQE), for distribution transformation and manifold embedding with the ability to choose the embedding distribution. QQE, which uses the concept of quantile-quantile plot from visual statistical tests, can transform the distribution of data to any theoretical desired distribution or empirical reference sample. Moreover, QQE gives the user a choice of embedding distribution in embedding the manifold of data into the low dimensional embedding space. It can also be used for modifying the embedding distribution of other dimensionality reduction methods, such as PCA, t-SNE, and deep metric learning, for better representation or visualization of data. We propose QQE in both unsupervised and supervised forms. QQE can also transform a distribution to either an exact reference distribution or its shape. We show that QQE allows for better discrimination of classes in some cases. Our experiments on different synthetic and image datasets show the effectiveness of the proposed embedding method. </p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">TOOL-Gen-LLE</abbr>
    
  
  </div>

  <div id="ghojogh2021softimp" class="col-sm-8">
      <div class="title">
          
          Generative locally linear embedding: A module for manifold unfolding and visualization
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://www.researchgate.net/profile/Benyamin-Ghojogh" target="_blank">Benyamin Ghojogh</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://uwaterloo.ca/data-analytics/" target="_blank">Ali Ghodsi</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://uwaterloo.ca/scholar/karray" target="_blank">Fakhri Karray</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
        <em>Software Impacts</em>.

          
              9,
          
          
              (100105).
          

      

      
          Elsevier,
      
      
      
      
        2021.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2021-softimp-ghojogh-generative.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         Data often have nonlinear patterns in machine learning. One can unfold the nonlinear manifold of a dataset for low-dimensional visualization and feature extraction. Locally Linear Embedding (LLE) is a nonlinear spectral method for dimensionality reduction and manifold unfolding. It embeds data using the same linear reconstruction weights as in the input space. In this paper, we propose an open source module which not only implements LLE, but also includes implementations of two generative LLE algorithms whose linear reconstruction phases are stochastic. Using this module, one can generate as many manifold unfoldings as desired for data visualization or feature extraction.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="poorheravi2021cvis" class="col-sm-8">
      <div class="title">
          
          Acceleration of Large Margin Metric Learning for Nearest Neighbor Classification Using Triplet Mining and Stratified Sampling
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      Parisa Poorheravi,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://www.researchgate.net/profile/Benyamin-Ghojogh" target="_blank">Benyamin Ghojogh</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://uwaterloo.ca/scholar/vcgaudet" target="_blank">Vincent Gaudet</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://uwaterloo.ca/scholar/karray" target="_blank">Fakhri Karray</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
        <em>Journal of Computational Vision and Imaging Systems</em>.

          
              6,
          
          
              (1).
          

      

      
      
      
          Jan,
      
      
        2021.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2021-cvis-poorheravi-acceleration1.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://openjournals.uwaterloo.ca/index.php/vsl/article/view/3534" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         Metric learning is a technique in manifold learning to find a projection subspace for increasing and decreasing the inter- and intra-class variances, respectively. Some metric learning methods are based on triplet learning with anchor-positive-negative triplets. Large margin metric learning for nearest neighbor classification is one of the fundamental methods to do this. Recently, Siamese networks have been introduced with the triplet loss. Many triplet mining methods have been developed for Siamese nets; however, these techniques have not been applied on the triplets of large margin metric learning. In this work, inspired by the mining methods for Siamese nets, we propose several triplet mining techniques for large margin metric learning. Moreover, a hierarchical approach is proposed, for acceleration and scalability of optimization, where triplets are selected by stratified sampling in hierarchical hyper-spheres. We analyze the proposed methods on three publicly available datasets.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="sikaroudi2021icpr" class="col-sm-8">
      <div class="title">
          
          Batch-Incremental Triplet Sampling for Training Triplet Networks Using Bayesian Updating Theorem
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      Milad Sikaroudi,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://www.researchgate.net/profile/Benyamin-Ghojogh" target="_blank">Benyamin Ghojogh</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://uwaterloo.ca/scholar/karray" target="_blank">Fakhri Karray</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    <em>Mark Crowley</em>,
                  
                
              
        
            
            
              
                
                  
                    
                      and <a href="https://kimialab.uwaterloo.ca/kimia/index.php/h-r-tizhoosh/" target="_blank">H. R. Tizhoosh</a>.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>25th International Conference on Pattern Recognition (ICPR)</em>. 

      

      
          IEEE,
      
      
          Milan, Italy (virtual).
      
      
          Jan,
      
      
        2021.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2007.05610" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
      <a href="https://ieeexplore.ieee.org/document/9412478" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         Variants of Triplet networks are robust entities for learning a discriminative embedding subspace. There exist different triplet mining approaches for selecting the most suitable training triplets. Some of these mining methods rely on the extreme distances between instances, and some others make use of sampling. However, sampling from stochastic distributions of data rather than sampling merely from the existing embedding instances can provide more discriminative information. In this work, we sample triplets from distributions of data rather than from existing instances. We consider a multivariate normal distribution for the embedding of each class. Using Bayesian updating and conjugate priors, we update the distributions of classes dynamically by receiving the new mini-batches of training data. The proposed triplet mining with Bayesian updating can be used with any triplet-based loss function, e.g., triplet-loss or Neighborhood Component Analysis (NCA) loss. Accordingly, Our triplet mining approaches are called Bayesian Updating Triplet (BUT) and Bayesian Updating NCA (BUNCA), depending on which loss function is being used. Experimental results on two public datasets, namely MNIST and histopathology colorectal cancer (CRC), substantiate the effectiveness of the proposed triplet mining method.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ghojogh2020theoretical" class="col-sm-8">
      <div class="title">
          
          Theoretical Insights into the Use of Structural Similarity Index In Generative Models and Inferential Autoencoders
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://www.researchgate.net/profile/Benyamin-Ghojogh" target="_blank">Benyamin Ghojogh</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://uwaterloo.ca/scholar/karray" target="_blank">Fakhri Karray</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>International Conference on Image Analysis and Recognition (ICIAR-2020)</em>. 

      

      
          Springer,
      
      
          Póvoa de Varzim, Portugal (virtual).
      
      
          Jun,
      
      
        2020.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ghojogh2020weighted" class="col-sm-8">
      <div class="title">
          
          Weighted Fisher Discriminant Analysis in the Input and Feature Spaces
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://www.researchgate.net/profile/Benyamin-Ghojogh" target="_blank">Benyamin Ghojogh</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Milad Sikaroudi,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://kimialab.uwaterloo.ca/kimia/index.php/h-r-tizhoosh/" target="_blank">H.R. Tizhoosh</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://uwaterloo.ca/scholar/karray" target="_blank">Fakhri Karray</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>International Conference on Image Analysis and Recognition (ICIAR-2020)</em>. 

      

      
          Springer,
      
      
          Póvoa de Varzim, Portugal (virtual).
      
      
          Jun,
      
      
        2020.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ghojogh2019rda" class="col-sm-8">
      <div class="title">
          
          Generalized Subspace Learning by Roweis Discriminant Analysis
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://www.researchgate.net/profile/Benyamin-Ghojogh" target="_blank">Benyamin Ghojogh</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://uwaterloo.ca/scholar/karray" target="_blank">Fakhri Karray</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>International Conference on Image Analysis and Recognition (ICIAR-2020)</em>. 

      

      
          Springer,
      
      
          Póvoa de Varzim, Portugal (virtual).
      
      
          Jun,
      
      
        2020.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
      <a href="http://arxiv.org/abs/1910.05437" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         We present a new method which generalizes subspace learning based on eigenvalue and generalized eigenvalue problems. This method, Roweis Discriminant Analysis (RDA), is named after Sam Roweis to whom the field of subspace learning owes significantly. RDA is a family of infinite number of algorithms where Principal Component Analysis (PCA), Supervised PCA (SPCA), and Fisher Discriminant Analysis (FDA) are special cases. One of the extreme special cases, which we name Double Supervised Discriminant Analysis (DSDA), uses the labels twice, it is novel and has not appeared elsewhere. We propose a dual for RDA for some special cases. We also propose kernel RDA, generalizing kernel PCA, kernel SPCA, and kernel FDA, using both dual RDA and representation theory. Our theoretical analysis explains previously known facts such as why SPCA can use regression but FDA cannot, why PCA and SPCA have duals but FDA does not, why kernel PCA and kernel SPCA use kernel trick but kernel FDA does not, and why PCA is the best linear method for reconstruction. Roweisfaces and kernel Roweisfaces are also proposed generalizing eigenfaces, Fisherfaces, supervised eigenfaces, and their kernel variants. We also report experiments showing the effectiveness of RDA and kernel RDA on some benchmark datasets.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ghojogh2019ccai" class="col-sm-8">
      <div class="title">
          
          Instance Ranking and Numerosity Reduction Using Matrix Decompositionand Subspace Learning
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://www.researchgate.net/profile/Benyamin-Ghojogh" target="_blank">Benyamin Ghojogh</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>Canadian Conference on Artificial Intelligence</em>. 

      

      
          Springer’s Lecture Notes in Artificial Intelligence.,
      
      
          Kingston, ON, Canada.
      
      
      
        2019.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         One way to deal with the ever increasing amount of available data for processing is to rank data instances by usefulness and reduce the dataset size. In this work, we introduce a framework to achieve this using matrix decomposition and subspace learning. Our central contribution is a novel similarity measure for data instances that uses the basis obtained from matrix decomposition of the dataset. Using this similarity measure, we propose several related algorithms for ranking data instances and performing numerosity reduction. We then validate the effectiveness of these algorithms for data reduction on several datasets for classification, regression, and clustering tasks.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ghojogh2019llise" class="col-sm-8">
      <div class="title">
          
          Locally Linear Image Structural Embedding for Image Structure Manifold Learning
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://www.researchgate.net/profile/Benyamin-Ghojogh" target="_blank">Benyamin Ghojogh</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://uwaterloo.ca/scholar/karray" target="_blank">Fakhri Karray</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>International Conference on Image Analysis and Recognition (ICIAR-19)</em>. 

      

      
      
          Waterloo, Canada.
      
      
      
        2019.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ghojogh2019image" class="col-sm-8">
      <div class="title">
          
          Image Structure Subspace Learning Using Structural Similarity Index
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://www.researchgate.net/profile/Benyamin-Ghojogh" target="_blank">Benyamin Ghojogh</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://uwaterloo.ca/scholar/karray" target="_blank">Fakhri Karray</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>International Conference on Image Analysis and Recognition (ICIAR-19)</em>. 

      

      
      
          Waterloo, Canada.
      
      
      
        2019.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ghojogh2019pcassim" class="col-sm-8">
      <div class="title">
          
          Principal Component Analysis Using Structural Similarity Index for Images
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://www.researchgate.net/profile/Benyamin-Ghojogh" target="_blank">Benyamin Ghojogh</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://uwaterloo.ca/scholar/karray" target="_blank">Fakhri Karray</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>International Conference on Image Analysis and Recognition (ICIAR-19)</em>. 

      

      
      
          Waterloo, Canada.
      
      
      
        2019.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ghojogh2018psa" class="col-sm-8">
      <div class="title">
          
          Principal Sample Analysis for Data Reduction
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://www.researchgate.net/profile/Benyamin-Ghojogh" target="_blank">Benyamin Ghojogh</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>2018 IEEE International Conference on Big Knowledge (ICBK)</em>. 

      

      
      
          Singapore.
      
      
      
        2018.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>
            </div>
          
          
      

    
</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Mark  Crowley.
    Based on [*folio](https://github.com/bogoli/-folio) design. Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: April 09, 2025.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  



<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
