<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Mark  Crowley | Reinforcement Learning</title>
<meta name="description" content="">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/rlcourseS21/">

<!-- Theming-->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Mark</span>   Crowley
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/bio/">
                bio
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/contact/">
                contact
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/lab/">
                lab
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/news/">
                news
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/showcase/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/topics/">
                topics
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
      
          
            <h1 class="post-title">Reinforcement Learning</h1>
        
      
    <p class="post-description" style="border-bottom-style:dashed; border-bottom-color:lightgrey; border-bottom-width:1px; margin:1px; margin-bottom:10px;font-style: italic;">Spring 2021 - ECE 493 Topic 42</p>

      
      
      
      
      
      
        <p></p>
  </header>

  <article>
      
      <img src="/assets/img/teaching/ece493-logo.png" style="width: 300px; padding: 10px; float: right;">
      
    <p><strong><em>Note:</em></strong> This webpage is for a <strong>PREVIOUS OFFERING</strong> of the course ECE 493 Topic 42 - Reinforcement Learning, the particular schedules, details and assessment details do not necessarily apply to future terms.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Spring 2021</th>
      <th style="text-align: center"><strong>Instructor:</strong> Prof. Mark Crowley</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://uwaterloo.ca/scholar/mcrowley/classes/reinforcement-learning">Department Course Page</a></td>
      <td style="text-align: center"><a href="https://gingkoapp.com/course-457c-links-for-students">Updated List of Resources and Links</a></td>
    </tr>
    <tr>
      <td style="text-align: center">Piazza Discussions (TBD)</td>
      <td style="text-align: center"><a href="https://www.youtube.com/channel/UC6p1AJ7jKNFp6OB2MmAoWvA">Course YouTube Channel</a></td>
    </tr>
  </tbody>
</table>

<h3 id="course-description">Course Description</h3>

<p>Introduction to <a href="/reinforcement-learning/">Reinforcement Learning</a> (RL) theory and algorithms for learning decision-making policies in situations with uncertainty and limited information. Topics include Markov decision processes, classic exact/approximate RL algorithms such as value/policy iteration, Q-learning, State-action-reward-state-action (SARSA), Temporal Difference (TD) methods, policy gradients, actor-critic, and Deep RL such as Deep Q-Learning (DQN), Asynchronous Advantage Actor Critic (A3C), and Deep Deterministic Policy Gradient (DDPG).</p>

<h3 id="required-background">Required Background</h3>

<p>The course will use concepts from ECE 203 and ECE 307 on Bayesian Probability and Statistics, these will be reviewed but familiarity will help significantly. All other concepts needed for the course will be introduced directly. Examples, assignments and projects will depend on programming ability in Python.</p>

<h2 id="course-staff">Course Staff</h2>

<table>
  <thead>
    <tr>
      <th><strong>Instructor:</strong> Prof. Mark Crowley</th>
      <th>TA: TBD</th>
      <th>TA: TBD</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**Contact: **mcrowley@uwaterloo.ca (but for better results, use piazza or book an appointment via doodle)</td>
      <td><strong>Contact:</strong></td>
      <td><strong>Contact:</strong></td>
    </tr>
  </tbody>
</table>

<h2 id="general-course-structure">General Course Structure</h2>

<table>
  <thead>
    <tr>
      <th>Item</th>
      <th>Start</th>
      <th>End</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Live Lectures/Review Sessions (recorded and viewable later)</td>
      <td>TBD</td>
      <td>TBD</td>
    </tr>
    <tr>
      <td>Bookable Office  Hours (virtual, may or may not be recorded)</td>
      <td>TBD</td>
      <td>TBD</td>
    </tr>
    <tr>
      <td>¬†</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th>Item</th>
      <th>Weight Towards Final Grade</th>
      <th>Released (12:00  am)</th>
      <th>Due (11:59pm)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Assignment 1</td>
      <td>15%</td>
      <td>Tue, May 25</td>
      <td>Fri, Jun 11</td>
    </tr>
    <tr>
      <td>Midterm Exam (3 hr duration within window)</td>
      <td>20%</td>
      <td>Wed, Jun 23</td>
      <td>Fri, Jun 25</td>
    </tr>
    <tr>
      <td>Assignment 2</td>
      <td>15%</td>
      <td>Tue, Jun 15</td>
      <td>Fri, Jul 09</td>
    </tr>
    <tr>
      <td>Assignment 3</td>
      <td>20%</td>
      <td>Fri, Jul 9</td>
      <td>(Orig: Thu, Aug 5 +3%) Extended: Monday Aug 9</td>
    </tr>
    <tr>
      <td>Final Exam</td>
      <td>30%</td>
      <td>Thursday Aug 12 at 12:01am</td>
      <td>Saturday Aug 14 at 11:59pm</td>
    </tr>
  </tbody>
</table>

<h3 id="weekly-schedule">Weekly Schedule</h3>

<ul>
  <li>Most of the <strong>lectures are available already on Youtube</strong>, <em>each week</em> there are associated videos to watch along with readings from the Sutton and Barto textbook.</li>
  <li>A Detailed Course Calendar in the <a href="https://gingkoapp.com/course-457c-links-for-students">Course Gingko List</a>  will let you know which materials to study each week before Monday‚Äôs Live Session.
    <ul>
      <li><strong>Live Lecture/Review/Discussion Session - ** Every **TBD</strong> there will be a live session with the Prof which will be:
        <ul>
          <li><em>Either</em> a live lecture with new/revised content from what is on YouTube. Lectures will be recorded and available later on YouTube.</li>
          <li><em>Or</em> a live discussion session to review the content from last week‚Äôs self-study, working through problems, answering questions, breaking out for discussions. These sessions will be recorded and remain available on LEARN during the term.</li>
        </ul>
      </li>
      <li><strong>Tutorials:</strong> TBD - we‚Äôll see how, and if, we do tutorials this year with the new online arrangement.</li>
    </ul>
  </li>
  <li><strong><em>Bookable Office Hours -</em></strong>
    <ul>
      <li>These are regular, one-on-one (or a small group) meeting slots with the professor, <a href="https://doodle.com/mm/markcrowley/bookable-1on1-493t2">bookable via doodle</a> a few days ahead of time.</li>
    </ul>
  </li>
  <li><strong>TA office hours</strong> - to be determined</li>
</ul>

<h3 id="grade-breakdown">Grade Breakdown</h3>

<ul>
  <li>
    <p>Assignment 1: <strong>15%</strong> - Theoretical questions and calculations relating to probability, Multi-armed bandits and Markov Decision Processes. <em>Evaluation</em>: Some automated grading and manual grading of written answers.</p>
  </li>
  <li>
    <p>Assignment 2: <strong>15%</strong> - Implement fundamental, exact algorithms on simple domain such as grid world, Value Iteration, Policy Iteration. Implement RL algorithms for simple domain, SARSA, Q-Learning, Eligibility Traces. 
<em>Evaluation:</em> Short report with graphs, automated grading and code review.</p>
  </li>
  <li>
    <p>Midterm: <strong>20%</strong></p>
  </li>
  <li>
    <p>Assignment 3: <strong>20%</strong> - Implement Policy Gradient, Actor Critic, Value Function Approximations for larger RL domains. Implement RL algorithm for more complex domain using simple Deep Learning representation of value function.</p>

    <p><em>Evaluation:</em> Short report with graphs and code review. We might set-up at Kaggle in-class competition as well.</p>
  </li>
  <li>
    <p><strong>Final Exam</strong>: <strong>30%</strong> - all topics, leaning towards latter half of course</p>
  </li>
  <li>
    <p><strong>Group Work:</strong> Assignments can be done alone or in pairs. Groups will be setup on LEARN to signup for a pair. You do not need to maintain the same pair throughout the term.</p>
  </li>
</ul>

<h2 id="course-learning-objectives-and-topic-details">Course Learning Objectives and Topic Details</h2>

<h3 id="learning-objectives">Learning Objectives</h3>
<p>This course complements other AI courses in ECE by focussing on the
methods for representation and reasoning about uncertain knowledge for
the purposes of analysis and decision making. At each stage of the
course we will look at relevant applications of the methods being
discussed.</p>

<p>For example, in 2016 the AI program ‚ÄúAlphaGO‚Äù defeated human world class
players of the game Go for the first time. This system requires many
different methods to enable reasoning, probabilistic inference, planning
and decision optimization. In this course we will build up the
fundamental knowledge about these components and how they combine
together to make such systems possible.</p>

<ol>
  <li>Identify and Explain the component theoretical concepts of Reinforcement Learning systems.</li>
  <li>Implement or instantiate using a library any of the core Reinforcement Learning algorithms on a variety of domains.</li>
  <li>Evaluate the performance of a particular RL system on a given domain through proper experimental design, statistical analysis and visualization.</li>
</ol>

<h3 id="topics">Topics</h3>
<ol>
  <li>Motivation and Context
    <ul>
      <li>Importance of reasoning and decision making about uncertainty.</li>
      <li>Connection to Artificial Intelligence and Machine Learning.</li>
      <li>Probability review.</li>
    </ul>
  </li>
  <li>Decision making under uncertainty:
    <ul>
      <li>Multi-Armed Bandit (MAB) problems, Thompson Sampling.</li>
      <li>Markov Decision Processes (MDPs), Influence Diagram representation.</li>
    </ul>
  </li>
  <li>Solving MDPs
    <ul>
      <li>Theory, Bellman equations</li>
      <li>Relation to Control Theory</li>
      <li>Value Iteration, Policy Iteration</li>
    </ul>
  </li>
  <li>The Reinforcement Learning Problem
    <ul>
      <li>Approximately solving MDPs by interacting with the environment</li>
      <li>SARSA algorithm</li>
      <li>Q-learning algorithm</li>
    </ul>
  </li>
  <li>Temporal Difference Learning
    <ul>
      <li>Eligibility Traces</li>
      <li>TD(ùúÜ)</li>
    </ul>
  </li>
  <li>Direct Policy Search
    <ul>
      <li>Policy Gradients methods</li>
      <li>Actor-Critic methods</li>
    </ul>
  </li>
  <li>State Representation
    <ul>
      <li>Value Function Approximation</li>
      <li>Stochastic Gradient Descent</li>
    </ul>
  </li>
  <li>Basics of Neural Networks (review or refer to ECE657A content)
    <ul>
      <li>fully connected, multi-layer perceptrons</li>
      <li>supervised training, back-propagation</li>
      <li>regularization methods</li>
    </ul>
  </li>
  <li>Deep Reinforcement Learning
    <ul>
      <li>Deep Q- Networks (DQN)</li>
      <li>Experience replay buffers and mini-batch training</li>
      <li>A2C, DDPG, PPO</li>
    </ul>
  </li>
  <li>Other Challenges (brief)
    <ul>
      <li>Partially Observable MDPs (POMDPs)</li>
      <li>Multi-Agent RL (MARL)</li>
    </ul>
  </li>
  <li>Other ways to solve (PO)MDPs (if time permits)
    <ul>
      <li>Monte-Carlo Tree Search, Explaining AlphaGo</li>
      <li>Curiosity based learning</li>
      <li>Soft-Actor Critic</li>
    </ul>
  </li>
  <li>Wrap-up and Review</li>
</ol>

<h2 id="getting-help">Getting Help:</h2>

<ul>
  <li><strong>Discussion board:</strong>
    <ul>
      <li><em>Piazza</em> will be the main place for detailed discussion and questions. Students can post anonymously (from students only), post a collaborative answer and course staff can confirm these, post their own or run Live Q&amp;A events.</li>
      <li>Go there there and sign up with your UWaterloo email now!</li>
    </ul>
  </li>
  <li><strong>Pre-recorded Video Lectures:</strong> These will be made available on the <a href="https://www.youtube.com/channel/UC7vU2kP8oNwvr0GuAqoxYGA">course youtube channel</a>, and links from within Learn</li>
  <li><strong>LEARN Website:</strong> The main course content, announcements, grade tracking and materials will be made available on Learn. All registered students should see this in their LEARN courses.</li>
  <li><strong>Email the Teaching Assistant and Instructor:</strong> Office Hours will be arranged once term starts as needed.</li>
  <li><strong>AccessAbility Services :</strong> http://uwaterloo.ca/accessability-services
    <ul>
      <li>If you need any accommodation, assistance with exams, learning environment, assignments, talk to this office and they can help you set it up as securely and anonymously as possible.</li>
    </ul>
  </li>
</ul>

<h3 id="discussion-group-protocols">Discussion Group Protocols</h3>

<ul>
  <li>Posts on Piazza can be public or anonymous to your classmates, but they will <em>never</em> be anonymous to the TAs and Instructor.</li>
  <li>Be kind. Assume the best, not the worst. Think before you hit enter.</li>
  <li>Posts which are considered offensive, abusive, bullying, discriminatory to any group or person, will be made private or deleted and followed up with private discussion.</li>
  <li>If you feel there is inappropriate, hurtful behaviour occurring on the discussion forum, please notify the professor, TAs or department staff as you feel appropriate.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>If you really can‚Äôt get in touch with anyone and it is an emergency you can contact Prof. Crowley directory via Microsoft Teams messaging (please don‚Äôt abuse this though :</td>
          <td>)</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h2 id="course-resources">Course Resources</h2>

<h3 id="primary-reference-for-course">Primary Reference for Course</h3>

<p><strong>[SuttonBarto2018]</strong> - Reinforcement Learning: An Introduction. Book, free pdf of draft available.
http://incompleteideas.net/book/the-book-2nd.html</p>

<h3 id="other-useful-texts">Other Useful Texts</h3>

<p><strong>[Dimitrakakis2019]</strong> - Decision Making Under Uncertainty and Reinforcement Learning</p>

<p>http://www.cse.chalmers.se/~chrdimi/downloads/book.pdf</p>

<p><strong>[Ghavamzadeh2016]</strong> - Bayesian Reinforcement Learning: A Survey. Ghavamzadeh et al. 2016.
https://arxiv.org/abs/1609.04436</p>

<ul>
  <li>More probability notes online: https://compthinking.github.io/RLCourseNotes/</li>
</ul>

<h3 id="open-ai-reference-website">Open AI Reference Website</h3>

<p>This website is a great resource. It lays out concepts from start to finish. Once you get through the first half of our course, many of the concepts on this site will be familiar to you.</p>

<ul>
  <li>
    <p>Key Papers in Deep RL List - https://spinningup.openai.com/en/latest/spinningup/keypapers.html</p>
  </li>
  <li>
    <p>Fundamental RL Concepts Overview - The fundamentals of RL are briefly covered here. We will go into all this and more in detail in our course.
https://spinningup.openai.com/en/latest/spinningup/rl_intro.html</p>
  </li>
  <li>
    <p>Family Tree of Algorithms - Here, a list of algorithms at the cutting edge of RL as of 1 year ago to so, so it‚Äôs a good place to find out more. But in a fast growing field, it may be a bit out of date about the latest now.
https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html</p>
  </li>
</ul>

<h2 id="general-university-of-waterloo-guidelines">General University of Waterloo Guidelines:</h2>

<p><strong>Academic Integrity:</strong> In order to maintain a culture of academic
integrity, members of the University of Waterloo community are expected
to promote honesty, trust, fairness, respect and responsibility. Check
http://www.uwaterloo.ca/academicintegrity/ for more information.</p>

<p><strong>Grievance:</strong> A student who believes that a decision affecting some
aspect of his/her university life has been unfair or unreasonable may
have grounds for initiating a grievance. Read Policy 70, Student
Petitions and Grievances, Section 4,
http://www.adm.uwaterloo.ca/infosec/ Policies/policy70.htm. When in
doubt please be certain to contact the departments administrative
assistant who will provide further assistance.</p>

<p><strong>Discipline:</strong> A student is expected to know what constitutes academic
integrity‚Äîcheck http: //www.uwaterloo.ca/academicintegrity/ to avoid
committing an academic offence, and to take responsibility for his/her
actions. A student who is unsure whether an action constitutes an
offence, or who needs help in learning how to avoid offences (e.g.,
plagiarism, cheating) or about rules for group work/collaboration should
seek guidance from the course instructor, academic advisor, or the
undergraduate Associate Dean. For information on categories of offences
and types of penalties, students should refer to Policy 71, Student
Discipline, http://www.adm.uwaterloo.ca/infosec/Policies/policy71.htm.
For typical penalties check Guidelines for the Assessment of Penalties,
http://www.adm.uwaterloo.ca/infosec/guidelines/penaltyguidelines.htm.</p>

<p><strong>Appeals:</strong> A decision made or penalty imposed under Policy 70 (Student
Petitions and Grievances) (other than a petition) or Policy 71 (Student
Discipline) may be appealed if there is a ground. A student who believes
he/she has a ground for an appeal should refer to Policy 72 (Student
Appeals) http://www.adm.uwaterloo.ca/infosec/Policies/policy72.htm.</p>

<p><strong>Note for Students with Disabilities:</strong> The Office for Persons with
Disabilities (OPD), located in Needles Hall, Room 1132, collaborates
with all academic departments to arrange appropriate accommodations for
students with disabilities without compromising the academic integrity
of the curriculum. If you require academic accommodations to lessen the
impact of your disability, please register with the OPD at the beginning
of each academic term.</p>

  </article>


      

    
</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Mark  Crowley.
    Based on [*folio](https://github.com/bogoli/-folio) design. Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: April 09, 2025.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  



<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
