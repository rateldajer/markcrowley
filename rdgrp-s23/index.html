<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Mark  Crowley | Spring 2023 Reading Group</title>
<meta name="description" content="">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/rdgrp-s23/">

<!-- Theming-->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Mark</span>   Crowley
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/bio/">
                bio
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/contact/">
                contact
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/lab/">
                lab
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/news/">
                news
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/showcase/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/topics/">
                topics
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
      
          
            <h1 class="post-title">Spring 2023 Reading Group</h1>
          
      
    <p class="post-description" style="border-bottom-style:dashed; border-bottom-color:lightgrey; border-bottom-width:1px; margin:1px; margin-bottom:10px;font-style: italic;">Reading group on Transformers in the lab in Spring 2023</p>

      
      
      
      
      
      
        <p></p>
  </header>

  <article>
      
    <h2>Reading Groups Tips</h2>
<p>In a <strong><a href="/reading-groups/">reading group</a></strong> everyone takes turns <em>leading</em> discussion of a paper each week. Leading discussion can be as simple as having your own annotated notes on Hypothes.is to share and start discussion as we go through it together. Or it could be more involved, including <em>making slides</em> to present your overview of the paper’s contributions, highlights and weak points.</p>

<hr />

<h2>Spring 2023 - Transformers Reading Group</h2>

<h3>Motivation</h3>

<p>AI research has been undergoing since the dawn of computer science itself, and Deep Learning has seen an uninterrupted, and accelerating wave of advancing abilities for over 12 years since the public breakthroughs of CNNs in 2012. Yet still, many people, including AI/ML researchers have been surprised at the abilities of the generative models that have been released since summer 2022 by OpenAI, Facebook, Google and others. The recent systems all rely in various ways on the Transformer model (missing reference).</p>

<h3>Resources</h3>
<p>This github page has a quite extensive list of papers and references on the topic so seems as good a place as any to start:</p>
<ul><li>
<a href="https://github.com/Hannibal046/Awesome-LLM#milestone-papers">Awesome LLM Milestone Papers</a>
</li></ul>

<hr />

<p>See the links and notes on paper we have <strong>done</strong> in previous meetings, obtain the link for the <strong>next</strong> paper or look at planned <strong>upcoming</strong> or <strong>potential</strong> future papers, feel free to suggest others or changes in the upcoming order.</p>

<p><b>Jump to stage:</b>   <a href="#rd-next">next</a>  ~    <a href="#rd-done">done</a>  ~    <a href="#rd-upcoming">upcoming</a>  ~    <a href="#rd-potential">potential</a></p>

<div class="publications by year">

  
  <h2 class="year"><a name="rd-next">next</a></h2>
    <br /><br /> 
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  

  
  <h2 class="year"><a name="rd-done">done</a></h2>
    <br /><br /> 
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="yang2023arxiv" class="col-sm-8">
      <div class="title">
           <b>[8]</b> 
          Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      Jingfeng Yang,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Hongye Jin,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Ruixiang Tang,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Xiaotian Han,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Qizhang Feng,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Haoming Jiang,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Bing Yin,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      and Xia Hu.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
        <em>Arxiv Preprint</em>.

          
          

      

      
      
      
      
        2023.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
    
    
    
      
      <a href="https://dl.acm.org/doi/pdf/10.1145/3649506" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://doi.org/10.1145/3649506" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
        
        
          <a class="abstract btn btn-sm z-depth-0" role="button">Notes</a>
        
    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    
        
        
        <div class="abstract hidden">
            <p><b>Note:</b> I’m wary of some of their intro, "BERT models started to disappear" it’s only been a year or two. They have a very nice overview figure. This recent review paper gives content on what tasks GPT style decoder-only LLMs are good for and which they are not (most tasks in fact). </p>
          </div>
        
    

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>
  
      <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="tay2021acl" class="col-sm-8">
      <div class="title">
           <b>[7]</b> 
          Are Pretrained Convolutions Better than Pretrained Transformers?
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      Yi Tay,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Mostafa Dehghani,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Jai Prakash Gupta,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Vamsi Aribandi,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Dara Bahri,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Zhen Qin,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      and Donald Metzler.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>. 

      

      
          Association for Computational Linguistics,
      
      
          Online..
      
      
          Aug,
      
      
        2021.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2021-acl-tay-are%20pretrained" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://aclanthology.org/2021.acl-long.335" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    
      <a href="https://hyp.is/go?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2105.03322.pdf&amp;group=__world__" class="btn btn-sm z-depth-0" role="button" target="_blank">Hypoth</a>
    

    
        
        
    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    
        
        
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p><b>Abstract:</b>
         In the era of pre-trained language models, Transformers are the de facto choice of model architectures. While recent research has shown promise in entirely convolutional, or CNN, architectures, they have not been explored using the pre-train-fine-tune paradigm. In the context of language models, are convolutional models competitive to Transformers when pre-trained? This paper investigates this research question and presents several interesting findings. Across an extensive set of experiments on 8 datasets/tasks, we find that CNN-based pre-trained models are competitive and outperform their Transformer counterpart in certain scenarios, albeit with caveats. Overall, the findings outlined in this paper suggest that conflating pre-training and architectural advances is misguided and that both advances should be considered independently. We believe our research paves the way for a healthy amount of optimism in alternative architectures.</p>
    </div>
    
  </div>
</div>
</li></ol>
  
      <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="thoppilan2022arxiv" class="col-sm-8">
      <div class="title">
           <b>[6]</b> 
          LaMDA: Language Models for Dialog Applications
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      Romal Thoppilan,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Daniel De Freitas,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Jamie Hall,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Noam Shazeer,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Apoorv Kulshreshtha,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Heng-Tze Cheng,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Alicia Jin,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Taylor Bos,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Leslie Baker,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Yu Du,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      YaGuang Li,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Hongrae Lee,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Huaixiu Steven Zheng,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Amin Ghafouri,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Marcelo Menegali,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Yanping Huang,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Maxim Krikun,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Dmitry Lepikhin,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      James Qin,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Dehao Chen,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Yuanzhong Xu,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Zhifeng Chen,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Adam Roberts,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Maarten Bosma,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Vincent Zhao,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Yanqi Zhou,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Chung-Ching Chang,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Igor Krivokon,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Will Rusch,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Marc Pickett,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Pranesh Srinivasan,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Laichee Man,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Kathleen Meier-Hellstern,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Meredith Ringel Morris,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Tulsee Doshi,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Renelito Delos Santos,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Toju Duke,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Johnny Soraker,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Ben Zevenbergen,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Vinodkumar Prabhakaran,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Mark Diaz,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Ben Hutchinson,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Kristen Olson,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Alejandra Molina,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Erin Hoffman-John,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Josh Lee,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Lora Aroyo,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Ravi Rajakumar,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Alena Butryna,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Matthew Lamm,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Viktoriya Kuzmina,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Joe Fenton,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Aaron Cohen,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Rachel Bernstein,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Ray Kurzweil,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Blaise Aguera-Arcas,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Claire Cui,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Marian Croak,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Ed Chi,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      and Quoc Le.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
        <em>Arxiv Preprint</em>.

          
          

      

      
      
      
      
        2022.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
    
    
    
      
      <a href="/assets/pdf/2022-arxiv-thoppilan-lamda" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    
    

    
        
        
          <a class="abstract btn btn-sm z-depth-0" role="button">Notes</a>
        
    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    
        
        
        <div class="abstract hidden">
            <p><b>Note:</b> Facebook enters the ring... </p>
          </div>
        
    

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>
  
      <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="gpt3" class="col-sm-8">
      <div class="title">
           <b>[5]</b> 
          Language Models are Few-Shot Learners
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      Tom Brown,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Benjamin Mann,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Nick Ryder,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Melanie Subbiah,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Jared D Kaplan,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Prafulla Dhariwal,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Arvind Neelakantan,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Pranav Shyam,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Girish Sastry,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Amanda Askell,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Sandhini Agarwal,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Ariel Herbert-Voss,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Gretchen Krueger,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Tom Henighan,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Rewon Child,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Aditya Ramesh,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Daniel Ziegler,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Jeffrey Wu,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Clemens Winter,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Chris Hesse,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Mark Chen,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Eric Sigler,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Mateusz Litwin,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Scott Gray,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Benjamin Chess,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Jack Clark,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Christopher Berner,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Sam McCandlish,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Alec Radford,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Ilya Sutskever,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      and Dario Amodei.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>Advances in Neural Information Processing Systems</em>. 

      

      
      
          Virtual..
      
      
      
        2020.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2005.14165" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
      
      <a href="/assets/pdf/2020-neurips-brown-language.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    
      <a href="https://hyp.is/go?url=https%3A%2F%2Fpapers.nips.cc%2Fpaper_files%2Fpaper%2F2020%2Ffile%2F1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf&amp;group=__world__" class="btn btn-sm z-depth-0" role="button" target="_blank">Hypoth</a>
    

    
        
          <a class="abstract btn btn-sm z-depth-0" role="button">Notes</a>
        
        
          <a class="abstract btn btn-sm z-depth-0" role="button">Notes</a>
        
    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    
        
        <div class="abstract hidden">
            <p><b>Note:</b> Introduction of the GPT-3 model. </p>
        </div>
        
        
        <div class="abstract hidden">
            <p><b>Note:</b> Introduction of the GPT-3 model. </p>
          </div>
        
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p><b>Abstract:</b>
         We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="gpt2" class="col-sm-8">
      <div class="title">
           <b>[5]</b> 
          Language Models are Unsupervised Multitask Learners
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      Alec Radford,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Jeff Wu,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Rewon Child,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      David Luan,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Dario Amodei,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      and Ilya Sutskever.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em></em>. 

      

      
      
      
      
        2019.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2019-arxiv-radford-language" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    
      <a href="https://hyp.is/go?url=https%3A%2F%2Fd4mucfpksywv.cloudfront.net%2Fbetter-language-models%2Flanguage-models.pdf&amp;group=__world__" class="btn btn-sm z-depth-0" role="button" target="_blank">Hypoth</a>
    

    
        
        
    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    
        
        
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p><b>Abstract:</b>
         Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations. </p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="lewis2019arxiv" class="col-sm-8">
      <div class="title">
           <b>[5]</b> 
          Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      Mike Lewis,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Yinhan Liu,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Naman Goyal,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Marjan Ghazvininejad,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Abdelrahman Mohamed,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Omer Levy,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Ves Stoyanov,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      and Luke Zettlemoyer.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:1910.13461</em>.

          
          

      

      
      
      
      
        2019.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
    
    
    
      
      <a href="/assets/pdf/2019-arxiv-lewis-bart" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    
    

    
        
        
          <a class="abstract btn btn-sm z-depth-0" role="button">Notes</a>
        
    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    
        
        
        <div class="abstract hidden">
            <p><b>Note:</b> This paper builds on the success of BERT but maintaining full encoder-decoder framework. </p>
          </div>
        
    

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>
  
      <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="devlin2019naaclhlt" class="col-sm-8">
      <div class="title">
           <b>[4]</b> 
          BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      Jacob Devlin,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Ming-Wei Chang,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Kenten Lee,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      and Kristina Toutanova.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>Proceedings of NAACL-HLT</em>. 

      

      
      
      
      
        2019.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
    
    
    
      
      <a href="/assets/pdf/2019-naaclhlt-kenton-bert%20pre-training%20of%20deep%20bidirectional%20transformers%20for%20language%20understanding" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    
    

    
        
        
          <a class="abstract btn btn-sm z-depth-0" role="button">Notes</a>
        
    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    
        
        
        <div class="abstract hidden">
            <p><b>Note:</b> The original paper for the BERT transformer model for NLP tasks. </p>
          </div>
        
    

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>
  
      <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="liu2019" class="col-sm-8">
      <div class="title">
           <b>[3]</b> 
          RoBERTa: A Robustly Optimized BERT Pretraining Approach
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      Yinhan Liu,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Myle Ott,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Naman Goyal,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Jingfei Du,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Mandar Joshi,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Danqi Chen,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Omer Levy,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Mike Lewis,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Luke Zettlemoyer,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      and Veselin Stoyanov.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
        <em></em>.

          
          

      

      
      
      
          Jul,
      
      
        2019.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2019-arxiv-liu-roberta.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://arxiv.org/pdf/1907.11692.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
        
        
          <a class="abstract btn btn-sm z-depth-0" role="button">Notes</a>
        
    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    
        
        
        <div class="abstract hidden">
            <p><b>Note:</b> Where does the annotation show up? anywhere? </p>
          </div>
        
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p><b>Abstract:</b>
         Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="radfordopenai" class="col-sm-8">
      <div class="title">
           <b>[3]</b> 
          Improving Language Understanding by Generative Pre-Training
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      Alec Radford,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Karthik Narasimhan,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Tim Salimans,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      and Ilya Sutskever.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
        <em>Preprint</em>.

          
          

      

      
      
      
      
        2018.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
    
    
    
      
      <a href="/assets/pdf/2018-openai-radford-improving" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    
    

    
        
        
          <a class="abstract btn btn-sm z-depth-0" role="button">Notes</a>
        
    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    
        
        
        <div class="abstract hidden">
            <p><b>Note:</b> The founding paper for GPT 1.0 posted online as a preprint. </p>
          </div>
        
    

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>
  
      <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Trnsfrmr</abbr>
    
  
  </div>

  <div id="vaswani2017neurips" class="col-sm-8">
      <div class="title">
           <b>[2]</b> 
          Attention is All you Need
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      Ashish Vaswani,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Noam Shazeer,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Niki Parmar,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Jakob Uszkoreit,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Llion Jones,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Aidan N Gomez,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Łukasz Kaiser,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      and Illia Polosukhin.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>Advances in Neural Information Processing Systems</em>. 

      

      
      
          Long Beach California, USA.
      
      
          Dec,
      
      
        2017.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2017-neurips-vaswani-attention.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    
      <a href="https://hyp.is/go?url=https%3A%2F%2Farxiv.org%2Fpdf%2F1706.03762.pdf&amp;group=__world__" class="btn btn-sm z-depth-0" role="button" target="_blank">Hypoth</a>
    

    
        
        
          <a class="abstract btn btn-sm z-depth-0" role="button">Notes</a>
        
    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    
        
        
        <div class="abstract hidden">
            <p><b>Note:</b> The original paper that introduced the Transformers architecture.  The stack of modules of the original transformer are quite similar to multiple subsequent layers of a CNN rather than the CNN filters. <br /><b>Questions</b><br /> <ol> <li> Why do they "stack" the modules in the original transformer?  </li> <li> How does the output of one module feed into the next in the stack?  </li> <li>Is there a notion of "sets" and "subsets" somewhere in this definition? Relations being learned amongst sets of non-local symbols?  </li> <li>Multi-head : where is the multi-part? It’s not the QKV parts, it’s a set of <em>h</em> copies of the attention module. (this is well hidden) </li> <li>"Pair-of-pairs" we discussed that the <em>h</em> copies of the the attention mechanism are considering pairs-of-pairs of symbol outputs or attention outputs, is it really though? Or are they independent filters as in CNNs?  </li> <li><b>The Big Question:</b> why does it work so well? It’s not just "more weights is better", the architecture matters.  </li> </ol> </p>
          </div>
        
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p><b>Abstract:</b>
         The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.</p>
    </div>
    
  </div>
</div>
</li></ol>
  
      <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ghojogh2020osfpreprint" class="col-sm-8">
      <div class="title">
           <b>[1]</b> 
          Attention Mechanism, Transformers, BERT, and GPT: Tutorial and Survey
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://www.researchgate.net/profile/Benyamin-Ghojogh" target="_blank">Benyamin Ghojogh</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      and <a href="https://uwaterloo.ca/data-analytics/" target="_blank">Ali Ghodsi</a>.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      

      
      
      
          Dec,
      
      
        2020.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2020-osfpreprin-ghojogh-attention" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://osf.io/m6gcn" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
        
        
    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    
        
        
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p><b>Abstract:</b>
         This is a tutorial and survey paper on the attention mechanism, transformers, BERT, and GPT. We first explain attention mechanism, sequence-to-sequence model without and with attention, self-attention, and attention in different areas such as natural language processing and computer vision. Then, we explain transformers which do not use any recurrence. We explain all the parts of encoder and decoder in the transformer, including positional encoding, multihead self-attention and cross-attention, and masked multihead attention. Thereafter, we introduce the Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT) as the stacks of encoders and decoders of transformer, respectively. We explain their characteristics and how they work.</p>
    </div>
    
  </div>
</div>
</li></ol>
  

  
  <h2 class="year"><a name="rd-upcoming">upcoming</a></h2>
    <br /><br /> 
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  

  
  <h2 class="year"><a name="rd-potential">potential</a></h2>
    <br /><br /> 
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="shanahan2022arxiv" class="col-sm-8">
      <div class="title">
           <b>[9]</b> 
          Talking About Large Language Models
      </div>
      <div class="author">
        
            
            
              
                
                   Murray Shanahan.
                
              
        
      </div>

      <div class="periodical">
      
        <em>Arxiv Preprint</em>.

          
          

      

      
      
      
          Dec,
      
      
        2022.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2212.03551" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
      
      <a href="/assets/pdf/2022-arxiv-shanahan-talking" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://arxiv.org/pdf/2212.03551.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
        
        
    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    
        
        
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p><b>Abstract:</b>
         Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as "knows", "believes", and "thinks", when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="taylor2022arxiv" class="col-sm-8">
      <div class="title">
           <b>[99]</b> 
          Galactica: A Large Language Model for Science
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      Ross Taylor,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Marcin Kardas,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Guillem Cucurull,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Thomas Scialom,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Anthony Hartshorn,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Elvis Saravia,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Andrew Poulton,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Viktor Kerkez,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      and Robert Stojnic.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
        <em>Arxiv Preprint</em>.

          
          

      

      
      
      
      
        2022.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
    
    
    
      
      <a href="/assets/pdf/2022-arxiv-taylor-galactica" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    
    

    
        
        
          <a class="abstract btn btn-sm z-depth-0" role="button">Notes</a>
        
    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    
        
        
        <div class="abstract hidden">
            <p><b>Note:</b> A promising approach to scientific reasoning with LLMs. </p>
          </div>
        
    

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  
      <ol class="bibliography"></ol>
  



</div>


  </article>


      

    
</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Mark  Crowley.
    Based on [*folio](https://github.com/bogoli/-folio) design. Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: April 09, 2025.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  



<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
