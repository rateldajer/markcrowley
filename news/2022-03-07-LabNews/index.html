<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Mark  Crowley | Some great publications accepted in past four months.</title>
<meta name="description" content="">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/news/2022-03-07-LabNews/">

<!-- Theming-->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Mark</span>   Crowley
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/bio/">
                bio
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/contact/">
                contact
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/lab/">
                lab
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/news/">
                news
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/showcase/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/topics/">
                topics
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Some great publications accepted in past four months.</h1>
    
    <p class="post-description" style="border-bottom-style:dashed; border-bottom-color:lightgrey; border-bottom-width:1px; margin:1px; margin-bottom:10px;">Lab News for March 2022</p>
    

      
        
        <p class="post-description" style="border-bottom-style:solid; border-bottom-color:lightgrey; border-bottom-width:3px; margin:1px;">
        <b>DOMAINS</b>
        
            
            | <a href="/ai-for-science/">ai-for-science</a> 
        
            
            | <a href="/ai-for-chemistry/">ai-for-chemistry</a> 
        
            
            | <a href="/medical-imaging/">medical-imaging</a> 
        
            
            | <a href="/ai-for-games/">ai-for-games</a> 
        
        </p>
        
      
      
      
      
        <p></p>
        <p class="post-description" style="border-bottom-style:dashed; border-bottom-color:lightgrey; border-bottom-width:1px; margin:1px; margin-bottom:3px;">
        March 7, 2022 • publications
        </p>
  </header>

  <article class="post-content">
    <p>Members of the UWECEML lab have had a good couple months, with a few notable papers accepted to great venues.</p>

<h1 id="publications">Publications</h1>

<h3 id="aaai-paper-on-decentralized-multi-agent-reinforcement-learning">AAAI Paper on Decentralized Multi-Agent Reinforcement Learning</h3>
<p>At this year’s AAAI Conference on Artificial Intelligence in (location) Sriram Ganapathi Subramanian presented the paper:</p>

<blockquote>
  <p><span id="ganapathisubramanian2022aaai">Ganapathi Subramanian, S., Taylor, M., Crowley, M., &amp; Poupart, P. (2022). Decentralized Mean Field Games. <i>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI-2022)</i>, <i>36</i>(9), 9439–9447. https://doi.org/https://doi.org/10.1609/aaai.v36i9.21176</span></p>
</blockquote>

<p>You can read more about the research from <a href="https://www.ualberta.ca/science/news/2022/february/scaling-ai.html">this post about the paper</a> by one of the other co-authors Prof. Matt Taylor at the University of Alberta.</p>

<h3 id="canadian-ai-2022">Canadian AI 2022</h3>
<blockquote>
  <p><span id="Bellinger2022Balancing">Bellinger, C., Drozdyuk, A., Crowley, M., &amp; Tamblyn, I. (2022). Balancing Information with Observation Costs in Deep Reinforcement Learning. <i>Canadian Conference on Artificial Intelligence</i>, 12. https://caiac.pubpub.org/pub/0jmy7gpd</span></p>
</blockquote>

<p>This paper <a class="citation" href="#Bellinger2022Balancing">(Bellinger et al., 2022)</a> is called “Balancing Information with Observation Costs in Deep Reinforcement Learning” and it builds on other work <a class="citation" href="#beeler2022ieeeintsys">(Beeler et al., 2022)</a>, <a class="citation" href="#bellinger2022ai2ase">(Bellinger et al., 2022)</a>, <a class="citation" href="#bellinger2021canai">(Bellinger et al., 2021)</a> related to digital chemistry and material design where we attempt to use Reinforcement Learning to come up with better pathways for materials. This is a collaboration with the NRC. You can see the project page for <a href="/chemgymrl">ChemGymRL</a> for more information.</p>

<h3 id="neurips-workshop">Neurips Workshop</h3>
<blockquote>
  <p><span id="lee2021neuripsdeeprl">Lee, K. M., Ganapathi Subramanian, S., &amp; Crowley, M. (2021). Investigation of Independent Reinforcement Learning Algorithms in Multi-Agent Environments. <i>NeurIPS 2021 Deep Reinforcement Learning Workshop</i>, 15.</span></p>
</blockquote>

<p>This workshop paper <a class="citation" href="#lee2021neuripsdeeprl">(Lee et al., 2021)</a> and presentation was part of a project for undergraduate student Ken Ming Lee, who has worked in the lab as a URA multiple terms on RL algorithms and software development. The original idea to do an empirical study of RL algorithms in Multiagent setting flowed out of  results needed for Sriram’s PhD research. This paper is an empirical comparison of many single and multi-agent algorithms on a range of multi-agent planning domains. A longer version of this work has been submitted to Frontiers in AI journal for consideration <a class="citation" href="#lee2021frontai">(Lee et al., 2022)</a>.</p>


  </article>

  

      
        <div class="publications">
          <h2>References:</h2> 
        <ol class="bibliography"></ol>
        <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SubWorld</abbr>
    
  
  </div>

  <div id="beeler2022ieeeintsys" class="col-sm-8">
      <div class="title">
          
          Dynamic programming with partial information to overcome navigational uncertainty in a nautical environment
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      Chris Beeler,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Xinkai Li,
                    
                  
                
              
        
            
            
              
                
                  
                    <em>Mark Crowley</em>,
                  
                
              
        
            
            
              
                
                  
                    
                      Maia Fraser,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      and Isaac Tamblyn.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
        <em>IEEE Intelligent Systems</em>.

          
          

      

      
          IEEE,
      
      
      
      
        2022.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         Using a toy nautical navigation environment, we show that dynamic
programming can be used when only partial information about a partially
observed Markov decision process (POMDP) is known. By incorporating
uncertainty into our model, we show that navigation policies can be
constructed that maintain safety. Adding controlled sensing methods, we
show that these policies can also lower measurement costs at the same
time.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">MARLEmpircal</abbr>
    
  
  </div>

  <div id="lee2021frontai" class="col-sm-8">
      <div class="title">
          
          Investigation of Independent Reinforcement Learning Algorithms in Multi-Agent Environments
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://www.linkedin.com/in/km-lee/" target="_blank">Ken Ming Lee</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://sriramsubramanian.com/" target="_blank">Sriram Ganapathi Subramanian</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
        <em>Frontiers in Artificial Intelligence</em>.

          
          

      

      
      
      
          Sep,
      
      
        2022.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2022-frontai-lee-investigation.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://www.frontiersin.org/articles/10.3389/frai.2022.805823" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         Independent reinforcement learning algorithms have no theoretical guarantees for finding the best policy in multi-agent settings. However, in practice, prior works have reported good performance with independent algorithms in some domains and bad performance in others. Moreover, a comprehensive study of the strengths and weaknesses of independent algorithms is lacking in the literature. In this paper, we carry out an empirical comparison of the performance of independent algorithms on seven PettingZoo environments that span the three main categories of multi-agent environments, i.e., cooperative, competitive, and mixed. For the cooperative setting, we show that independent algorithms can perform on par with multi-agent algorithms in fully-observable environments, while adding recurrence improves the learning of independent algorithms in partially-observable environments. In the competitive setting, independent algorithms can perform on par or better than multi-agent algorithms, even in more challenging environments. We also show that agents trained via independent algorithms learn to perform well individually, but fail to learn to cooperate with allies and compete with enemies in mixed environments.

</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="Bellinger2022Balancing" class="col-sm-8">
      <div class="title">
          
          Balancing Information with Observation Costs in Deep Reinforcement Learning
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://web.cs.dal.ca/~bellinger/" target="_blank">Colin Bellinger</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Andriy Drozdyuk,
                    
                  
                
              
        
            
            
              
                
                  
                    <em>Mark Crowley</em>,
                  
                
              
        
            
            
              
                
                  
                    
                      and Isaac Tamblyn.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>Canadian Conference on Artificial Intelligence</em>. 

      

      
          Canadian Artificial Intelligence Association (CAIAC),
      
      
          Toronto, Ontario, Canada.
      
      
          May,
      
      
        2022.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2022-canai-bellinger-balancing.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
      
      <a href="https://www.dropbox.com/scl/fi/wv9jflxnja6pkol5vvfis/2022-canai-bellinger-balancing1.pdf?rlkey=z9zvsutypvbglzpih6a9dgwt5&raw=1" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
      <a href="https://caiac.pubpub.org/pub/0jmy7gpd" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    
      <a href="https://hyp.is/go?url=https%3A%2F%2Fassets.pubpub.org%2F99r5anzw%2F01652987005906.pdf&group=__world__" class="btn btn-sm z-depth-0" role="button" target="_blank">Hypoth</a>
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         
The use of Reinforcement Learning (RL) in scientific applications, such
as materials design and automated chemistry, is increasing. A major
challenge, however, lies in fact that measuring the state of the system
is often costly and time consuming in scientific applications, whereas
policy learning with RL requires a measurement after each time step. In
this work, we make the measurement costs explicit in the form of a
costed reward and propose the active-measure with costs framework that
enables off-the-shelf deep RL algorithms to learn a policy for both
selecting actions and determining whether or not to measure the state of
the system at each time step. In this way, the agents learn to balance
the need for information with the cost of information. Our results show
that when trained under this regime, the Dueling DQN and PPO agents can
learn optimal action policies whilst making up to 50% fewer state
measurements, and recurrent neural networks can produce a greater than
50% reduction in measurements. We postulate the these reduction can
help to lower the barrier to applying RL to real-world scientific
applications.
</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="bellinger2022ai2ase" class="col-sm-8">
      <div class="title">
          
          Scientific Discovery and the Cost of Measurement – Balancing Information and Cost in Reinforcement Learning
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://web.cs.dal.ca/~bellinger/" target="_blank">Colin Bellinger</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Andriy Drozdyuk,
                    
                  
                
              
        
            
            
              
                
                  
                    <em>Mark Crowley</em>,
                  
                
              
        
            
            
              
                
                  
                    
                      and Isaac Tamblyn.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>1st Annual AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE)</em>. 

      

      
      
      
          Feb,
      
      
        2022.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2112.07535" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         The use of reinforcement learning (RL) in scientific applications, such as materials design and automated chemistry, is increasing. A major challenge, however, lies in fact that measuring the state of the system is often costly and time consuming in scientific applications, whereas policy learning with RL requires a measurement after each time step. In this work, we make the measurement costs explicit in the form of a costed reward and propose a framework that enables off-the-shelf deep RL algorithms to learn a policy for both selecting actions and determining whether or not to measure the current state of the system at each time step. In this way, the agents learn to balance the need for information with the cost of information. Our results show that when trained under this regime, the Dueling DQN and PPO agents can learn optimal action policies whilst making up to 50% fewer state measurements, and recurrent neural networks can produce a greater than 50% reduction in measurements. We postulate the these reduction can help to lower the barrier to applying RL to real-world scientific applications.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">MARLEmpircal</abbr>
    
  
  </div>

  <div id="lee2021neuripsdeeprl" class="col-sm-8">
      <div class="title">
          
          Investigation of Independent Reinforcement Learning Algorithms in Multi-Agent Environments
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://www.linkedin.com/in/km-lee/" target="_blank">Ken Ming Lee</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://sriramsubramanian.com/" target="_blank">Sriram Ganapathi Subramanian</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>NeurIPS 2021 Deep Reinforcement Learning Workshop</em>. 

      

      
      
      
          Dec,
      
      
        2021.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2111.01100" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         Independent reinforcement learning algorithms have no theoretical guarantees for finding the best policy in multi-agent settings. However, in practice, prior works have reported good performance with independent algorithms in some domains and bad performance in others. Moreover, a comprehensive study of the strengths and weaknesses of independent algorithms is lacking in the literature. In this paper, we carry out an empirical comparison of the performance of independent algorithms on four PettingZoo environments that span the three main categories of multi-agent environments, i.e., cooperative, competitive, and mixed. We show that in fully-observable environments, independent algorithms can perform on par with multi-agent algorithms in cooperative and competitive settings. For the mixed environments, we show that agents trained via independent algorithms learn to perform well individually, but fail to learn to cooperate with allies and compete with enemies. We also show that adding recurrence improves the learning of independent algorithms in cooperative partially observable environments.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Amrl</abbr>
    
  
  </div>

  <div id="bellinger2021canai" class="col-sm-8">
      <div class="title">
          
          Active Measure Reinforcement Learning for Observation Cost Minimization: A framework for minimizing measurement costs in reinforcement learning
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://web.cs.dal.ca/~bellinger/" target="_blank">Colin Bellinger</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Rory Coles,
                    
                  
                
              
        
            
            
              
                
                  
                    <em>Mark Crowley</em>,
                  
                
              
        
            
            
              
                
                  
                    
                      and Isaac Tamblyn.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>Canadian Conference on Artificial Intelligence</em>. 

      

      
          Springer,
      
      
      
      
        2021.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2005.12697" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
      
      <a href="/assets/pdf/2021-canai-bellinger-active%20measure%20reinforcement.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         Markov Decision Processes (MDP) with explicit measurement cost are a class of en- vironments in which the agent learns to maximize the costed return. Here, we define the costed return as the discounted sum of rewards minus the sum of the explicit cost of measuring the next state. The RL agent can freely explore the relationship between actions and rewards but is charged each time it measures the next state. Thus, an op- timal agent must learn a policy without making a large number of measurements. We propose the active measure RL framework (Amrl) as a solution to this novel class of problem, and contrast it with standard reinforcement learning under full observability and planning under partially observability. We demonstrate that Amrl-Q agents learn to shift from a reliance on costly measurements to exploiting a learned transition model in order to reduce the number of real-world measurements and achieve a higher costed return. Our results demonstrate the superiority of Amrl-Q over standard RL methods, Q-learning and Dyna-Q, and POMCP for planning under a POMDP in environments with explicit measurement costs.</p>
    </div>
    
  </div>
</div>
</li></ol>
        </div>
      
</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Mark  Crowley.
    Based on [*folio](https://github.com/bogoli/-folio) design. Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: April 09, 2025.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  



<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
