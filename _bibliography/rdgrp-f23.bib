%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Mark at 2023-09-11 17:07:48 -0400 


%% Saved with string encoding Unicode (UTF-8) 



@article{wang2023nature,
	abbr = {AI4Science},
	abstract = {Artificial intelligence (AI) is being increasingly integrated into scientific discovery to augment and accelerate research, helping scientists to generate hypotheses, design experiments, collect and interpret large datasets, and gain insights that might not have been possible using traditional scientific methods alone. Here we examine breakthroughs over the past decade that include self-supervised learning, which allows models to be trained on vast amounts of unlabelled data, and geometric deep learning, which leverages knowledge about the structure of scientific data to enhance model accuracy and efficiency. Generative AI methods can create designs, such as small-molecule drugs and proteins, by analysing diverse data modalities, including images and sequences. We discuss how these methods can help scientists throughout the scientific process and the central issues that remain despite such advances. Both developers and users of AI tools need a better understanding of when such approaches need improvement, and challenges posed by poor data quality and stewardship remain. These issues cut across scientific disciplines and require developing foundational algorithmic approaches that can contribute to scientific understanding or acquire it autonomously, making them critical areas of focus for AI innovation.},
	annote = {This would be a great paper to look at for an update of the field. It is written by the organizers of the regular <a href=``https://ai4sciencecommunity.github.io/``>AI for Science workshop</a> held at multiple major conferences.},
	author = {Wang, Hanchen and Fu, Tianfan and Du, Yuanqi and Gao, Wenhao and Huang, Kexin and Liu, Ziming and Chandak, Payal and Liu, Shengchao and Van Katwyk, Peter and Deac, Andreea and Anandkumar, Anima and Bergen, Karianne and Gomes, Carla P. and Ho, Shirley and Kohli, Pushmeet and Lasenby, Joan and Leskovec, Jure and Liu, Tie-Yan and Manrai, Arjun and Marks, Debora and Ramsundar, Bharath and Song, Le and Sun, Jimeng and Tang, Jian and Veli{\v c}kovi{\'c}, Petar and Welling, Max and Zhang, Linfeng and Coley, Connor W. and Bengio, Yoshua and Zitnik, Marinka},
	date = {2023/08/01},
	date-added = {2023-09-11 13:13:07 -0400},
	date-modified = {2023-09-11 17:07:36 -0400},
	doi = {10.1038/s41586-023-06221-2},
	hypoth = {https://hyp.is/go?url=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs41586-023-06221-2&group=__world__},
	id = {Wang2023},
	isbn = {1476-4687},
	journal = {Nature},
	keywords = {next, ai-for-science, deep-learning, generative-models, large-language-models, nlp, ai-for-physics},
	number = {7972},
	order = {1},
	pages = {47--60},
	title = {Scientific discovery in the age of artificial intelligence},
	toread = {1},
	url = {https://doi.org/10.1038/s41586-023-06221-2},
	venue-short = {Nature},
	volume = {620},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1038/s41586-023-06221-2},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBTLi4vLi4vLi4vRHJvcGJveC93ZWJzaXRlX2Fzc2V0cy9tYXJrY3Jvd2xleS1jYS9wZGZzLzIwMjMtbmF0dXJlLXdhbmctc2NpZW50aWZpYy5wZGZPEQHOAAAAAAHOAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fMjAyMy1uYXR1cmUtd2FuZy1zY2llbnRpZmljLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAADAAUAAAogY3UAAAAAAAAAAAAAAAAABHBkZnMAAgBbLzpVc2VyczptY3Jvd2xleTpEcm9wYm94OndlYnNpdGVfYXNzZXRzOm1hcmtjcm93bGV5LWNhOnBkZnM6MjAyMy1uYXR1cmUtd2FuZy1zY2llbnRpZmljLnBkZgAADgBAAB8AMgAwADIAMwAtAG4AYQB0AHUAcgBlAC0AdwBhAG4AZwAtAHMAYwBpAGUAbgB0AGkAZgBpAGMALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAFlVc2Vycy9tY3Jvd2xleS9Ecm9wYm94L3dlYnNpdGVfYXNzZXRzL21hcmtjcm93bGV5LWNhL3BkZnMvMjAyMy1uYXR1cmUtd2FuZy1zY2llbnRpZmljLnBkZgAAEwABLwAAFQACAA///wAAAAgADQAaACQAegAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAJM}}

@article{willig2023acmpreprint,
	annote = {We didn't get to this paper in Spring 2023, so let's try again!},
	arxiv = {2308.13067},
	author = {Moritz Willig and Matej Ze{\v c}evi{\'c} and Devendra Singh Dhami and Kristian Kersting},
	date-added = {2023-06-12 21:48:13 -0400},
	date-modified = {2023-09-11 17:06:58 -0400},
	hypoth = {https://hyp.is/go?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2308.13067.pdf&group=__world__},
	in-website = {1},
	journal = {Transactions on Machine Learning Research},
	keywords = {large-language-models, upcoming},
	month = {August},
	order = {2},
	pdf = {2023-acmpreprin-willig-causal},
	title = {Causal Parrots: Large Language Models May Talk Causality But Are Not Causal},
	toread = {1},
	url = {https://arxiv.org/pdf/2308.13067.pdf},
	venue-short = {acmpreprint},
	year = {2023},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAvLi4vYXNzZXRzL3BkZi8yMDIzLWFjbXByZXByaW4td2lsbGlnLWNhdXNhbC5wZGZPEQG+AAAAAAG+AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fMjAyMy1hY21wcmVwcmluLXdpI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAMAAAogY3UAAAAAAAAAAAAAAAAAA3BkZgAAAgBSLzpVc2VyczptY3Jvd2xleTpyZXBvczptYXJrY3Jvd2xleS1jYTphc3NldHM6cGRmOjIwMjMtYWNtcHJlcHJpbi13aWxsaWctY2F1c2FsLnBkZgAOAEQAIQAyADAAMgAzAC0AYQBjAG0AcAByAGUAcAByAGkAbgAtAHcAaQBsAGwAaQBnAC0AYwBhAHUAcwBhAGwALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAFBVc2Vycy9tY3Jvd2xleS9yZXBvcy9tYXJrY3Jvd2xleS1jYS9hc3NldHMvcGRmLzIwMjMtYWNtcHJlcHJpbi13aWxsaWctY2F1c2FsLnBkZgATAAEvAAAVAAIAD///AAAACAANABoAJABWAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAhg=}}

@inproceedings{chen2021neurips,
	abbr = {DecTransfrmr},
	abstract = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
	archiveprefix = {arxiv},
	arxiv = {2106.01345},
	author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2023-05-28 14:42:06 -0400},
	date-modified = {2023-09-11 16:48:41 -0400},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	hypoth = {https://hyp.is/go?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2106.01345.pdf&group=__world__},
	in-website = {1},
	keywords = {upcoming, machine-learning, potential, reinforcement-learning, decision-transformers},
	month = {June},
	number = {arXiv:2106.01345},
	order = {3},
	pages = {15084--15097},
	primaryclass = {cs},
	shorttitle = {Decision Transformer},
	status = {1},
	title = {Decision Transformer: Reinforcement Learning via Sequence Modeling},
	toread = {1},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/7f489f642a0ddb10272b5c31057f0663-Paper.pdf},
	urldate = {2022-09-27},
	venue-short = {neurips},
	volume = {34},
	year = {2021},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAsLi4vYXNzZXRzL3BkZi8yMDIxLW5ldXJpcHMtY2hlbi1kZWNpc2lvbi5wZGZPEQG0AAAAAAG0AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8eMjAyMS1uZXVyaXBzLWNoZW4tZGVjaXNpb24ucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAMAAAogY3UAAAAAAAAAAAAAAAAAA3BkZgAAAgBPLzpVc2VyczptY3Jvd2xleTpyZXBvczptYXJrY3Jvd2xleS1jYTphc3NldHM6cGRmOjIwMjEtbmV1cmlwcy1jaGVuLWRlY2lzaW9uLnBkZgAADgA+AB4AMgAwADIAMQAtAG4AZQB1AHIAaQBwAHMALQBjAGgAZQBuAC0AZABlAGMAaQBzAGkAbwBuAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBNVXNlcnMvbWNyb3dsZXkvcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIxLW5ldXJpcHMtY2hlbi1kZWNpc2lvbi5wZGYAABMAAS8AABUAAgAP//8AAAAIAA0AGgAkAFMAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAACCw==},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/7f489f642a0ddb10272b5c31057f0663-Paper.pdf}}

@comment{BibDesk Smart Groups{
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Keywords</string>
				<key>value</key>
				<string>next</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>1 - next</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Keywords</string>
				<key>value</key>
				<string>upcoming</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>2 - upcoming</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Keywords</string>
				<key>value</key>
				<string>potential</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>3 - potential</string>
	</dict>
</array>
</plist>
}}
