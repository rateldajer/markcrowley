%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Mark at 2023-09-06 17:34:59 -0400 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{gpt2,
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations. },
	author = {Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
	date-added = {2023-07-04 12:55:43 -0400},
	date-modified = {2023-09-06 17:34:01 -0400},
	hypoth = {https://hyp.is/go?url=https%3A%2F%2Fd4mucfpksywv.cloudfront.net%2Fbetter-language-models%2Flanguage-models.pdf&group=__world__},
	keywords = {done},
	order = {6},
	pdf = {2019-arxiv-radford-language},
	title = {Language Models are Unsupervised Multitask Learners},
	url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf},
	venue-short = {arxiv},
	year = {2019},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBLLi4vLi4vLi4vLi4vcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDE5LWFyeGl2LXJhZGZvcmQtbGFuZ3VhZ2UucGRmTxEBtgAAAAABtgACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////HzIwMTktYXJ4aXYtcmFkZm9yZC1sYW5ndWFnZS5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAABAAFAAAKIGN1AAAAAAAAAAAAAAAAAANwZGYAAAIAUC86VXNlcnM6bWNyb3dsZXk6cmVwb3M6bWFya2Nyb3dsZXktY2E6YXNzZXRzOnBkZjoyMDE5LWFyeGl2LXJhZGZvcmQtbGFuZ3VhZ2UucGRmAA4AQAAfADIAMAAxADkALQBhAHIAeABpAHYALQByAGEAZABmAG8AcgBkAC0AbABhAG4AZwB1AGEAZwBlAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBOVXNlcnMvbWNyb3dsZXkvcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDE5LWFyeGl2LXJhZGZvcmQtbGFuZ3VhZ2UucGRmABMAAS8AABUAAgAP//8AAAAIAA0AGgAkAHIAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAACLA==},
	bdsk-url-1 = {https://paperswithcode.com/paper/language-models-are-unsupervised-multitask},
	bdsk-url-2 = {https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe}}

@inproceedings{gpt3,
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	address = {Virtual.},
	note = {Introduction of the GPT-3 model.},
	arxiv = {2005.14165},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2023-07-04 12:51:06 -0400},
	date-modified = {2023-09-06 17:34:03 -0400},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	hypoth = {https://hyp.is/go?url=https%3A%2F%2Fpapers.nips.cc%2Fpaper_files%2Fpaper%2F2020%2Ffile%2F1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf&group=__world__},
	keywords = {large-language-models, gpt, transformers; done},
	order = {6},
	pages = {1877--1901},
	pdf = {2020-neurips-brown-language},
	title = {Language Models are Few-Shot Learners},
	toread = {1},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	venue-short = {neurips},
	volume = {33},
	year = {2020},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBLLi4vLi4vLi4vLi4vcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIwLW5ldXJpcHMtYnJvd24tbGFuZ3VhZ2UucGRmTxEBtgAAAAABtgACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////HzIwMjAtbmV1cmlwcy1icm93bi1sYW5ndWFnZS5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAABAAFAAAKIGN1AAAAAAAAAAAAAAAAAANwZGYAAAIAUC86VXNlcnM6bWNyb3dsZXk6cmVwb3M6bWFya2Nyb3dsZXktY2E6YXNzZXRzOnBkZjoyMDIwLW5ldXJpcHMtYnJvd24tbGFuZ3VhZ2UucGRmAA4AQAAfADIAMAAyADAALQBuAGUAdQByAGkAcABzAC0AYgByAG8AdwBuAC0AbABhAG4AZwB1AGEAZwBlAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBOVXNlcnMvbWNyb3dsZXkvcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIwLW5ldXJpcHMtYnJvd24tbGFuZ3VhZ2UucGRmABMAAS8AABUAAgAP//8AAAAIAA0AGgAkAHIAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAACLA==},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}}

@inproceedings{tay2021acl,
	abstract = {In the era of pre-trained language models, Transformers are the de facto choice of model architectures. While recent research has shown promise in entirely convolutional, or CNN, architectures, they have not been explored using the pre-train-fine-tune paradigm. In the context of language models, are convolutional models competitive to Transformers when pre-trained? This paper investigates this research question and presents several interesting findings. Across an extensive set of experiments on 8 datasets/tasks, we find that CNN-based pre-trained models are competitive and outperform their Transformer counterpart in certain scenarios, albeit with caveats. Overall, the findings outlined in this paper suggest that conflating pre-training and architectural advances is misguided and that both advances should be considered independently. We believe our research paves the way for a healthy amount of optimism in alternative architectures.},
	address = {Online.},
	author = {Tay, Yi and Dehghani, Mostafa and Gupta, Jai Prakash and Aribandi, Vamsi and Bahri, Dara and Qin, Zhen and Metzler, Donald},
	booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	date-added = {2023-07-04 12:15:30 -0400},
	date-modified = {2023-09-06 17:34:15 -0400},
	doi = {10.18653/v1/2021.acl-long.335},
	eprint = {2105.03322},
	hypoth = {https://hyp.is/go?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2105.03322.pdf&group=__world__},
	keywords = {done},
	month = August,
	order = {7},
	pages = {4349--4359},
	pdf = {2021-acl-tay-are pretrained},
	publisher = {Association for Computational Linguistics},
	title = {Are Pretrained Convolutions Better than Pretrained Transformers?},
	url = {https://aclanthology.org/2021.acl-long.335},
	venue-short = {ACL},
	year = {2021},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBLLi4vLi4vLi4vLi4vcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIxLWFjbC10YXktYXJlIHByZXRyYWluZWQucGRmTxEBtgAAAAABtgACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////HzIwMjEtYWNsLXRheS1hcmUgcHJldHJhaW5lZC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAABAAFAAAKIGN1AAAAAAAAAAAAAAAAAANwZGYAAAIAUC86VXNlcnM6bWNyb3dsZXk6cmVwb3M6bWFya2Nyb3dsZXktY2E6YXNzZXRzOnBkZjoyMDIxLWFjbC10YXktYXJlIHByZXRyYWluZWQucGRmAA4AQAAfADIAMAAyADEALQBhAGMAbAAtAHQAYQB5AC0AYQByAGUAIABwAHIAZQB0AHIAYQBpAG4AZQBkAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBOVXNlcnMvbWNyb3dsZXkvcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIxLWFjbC10YXktYXJlIHByZXRyYWluZWQucGRmABMAAS8AABUAAgAP//8AAAAIAA0AGgAkAHIAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAACLA==},
	bdsk-url-1 = {https://aclanthology.org/2021.acl-long.335},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.335}}

@article{willig2023acmpreprint,
	author = {MORITZ WILLIG and MATEJ ZE{\v C}EVI{\'C} and DEVENDRA SINGH DHAMI and KRISTIAN KERSTING},
	date-added = {2023-06-12 21:48:13 -0400},
	date-modified = {2023-09-06 17:34:51 -0400},
	in-website = {1},
	journal = {Arxiv Preprint},
	keywords = {transformers, large-language-models, nlp; potential},
	order = {9},
	pdf = {2023-acmpreprin-willig-causal},
	title = {Causal Parrots: Large Language Models May Talk Causality But Are Not Causal},
	toread = {1},
	url = {https://www.matej-zecevic.de/research/assets/Causal_Parrots__Large_Language_Models_May_Talk_Causality_But_Are_Not_Causal_.pdf},
	venue-short = {acmpreprint},
	year = {2023},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBNLi4vLi4vLi4vLi4vcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIzLWFjbXByZXByaW4td2lsbGlnLWNhdXNhbC5wZGZPEQG+AAAAAAG+AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fMjAyMy1hY21wcmVwcmluLXdpI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAAEAAUAAAogY3UAAAAAAAAAAAAAAAAAA3BkZgAAAgBSLzpVc2VyczptY3Jvd2xleTpyZXBvczptYXJrY3Jvd2xleS1jYTphc3NldHM6cGRmOjIwMjMtYWNtcHJlcHJpbi13aWxsaWctY2F1c2FsLnBkZgAOAEQAIQAyADAAMgAzAC0AYQBjAG0AcAByAGUAcAByAGkAbgAtAHcAaQBsAGwAaQBnAC0AYwBhAHUAcwBhAGwALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAFBVc2Vycy9tY3Jvd2xleS9yZXBvcy9tYXJrY3Jvd2xleS1jYS9hc3NldHMvcGRmLzIwMjMtYWNtcHJlcHJpbi13aWxsaWctY2F1c2FsLnBkZgATAAEvAAAVAAIAD///AAAACAANABoAJAB0AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAjY=}}

@article{shanahan2022arxiv,
	abstract = {Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as "knows", "believes", and "thinks", when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.},
	arxiv = {2212.03551},
	author = {Murray Shanahan},
	date-added = {2023-06-12 21:39:40 -0400},
	date-modified = {2023-09-06 17:34:46 -0400},
	eprint = {2212.03551},
	in-website = {1},
	journal = {Arxiv Preprint},
	keywords = {potential},
	month = {12},
	order = {9},
	pdf = {2022-arxiv-shanahan-talking},
	title = {Talking About Large Language Models},
	toread = {1},
	url = {https://arxiv.org/pdf/2212.03551.pdf},
	venue-short = {arxiv},
	year = {2022},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBLLi4vLi4vLi4vLi4vcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIyLWFyeGl2LXNoYW5haGFuLXRhbGtpbmcucGRmTxEBtgAAAAABtgACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////HzIwMjItYXJ4aXYtc2hhbmFoYW4tdGFsa2luZy5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAABAAFAAAKIGN1AAAAAAAAAAAAAAAAAANwZGYAAAIAUC86VXNlcnM6bWNyb3dsZXk6cmVwb3M6bWFya2Nyb3dsZXktY2E6YXNzZXRzOnBkZjoyMDIyLWFyeGl2LXNoYW5haGFuLXRhbGtpbmcucGRmAA4AQAAfADIAMAAyADIALQBhAHIAeABpAHYALQBzAGgAYQBuAGEAaABhAG4ALQB0AGEAbABrAGkAbgBnAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBOVXNlcnMvbWNyb3dsZXkvcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIyLWFyeGl2LXNoYW5haGFuLXRhbGtpbmcucGRmABMAAS8AABUAAgAP//8AAAAIAA0AGgAkAHIAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAACLA==},
	bdsk-file-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBdLi4vLi4vLi4vLi4vcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIyLWFyeGl2LXNoYW5haGFuLXRhbGtpbmctYW5ub3RhdGVkMjAyMzA2MTIucGRmTxEB/gAAAAAB/gACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////HzIwMjItYXJ4aXYtc2hhbmFoYSNGRkZGRkZGRi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAABAAFAAAKIGN1AAAAAAAAAAAAAAAAAANwZGYAAAIAYi86VXNlcnM6bWNyb3dsZXk6cmVwb3M6bWFya2Nyb3dsZXktY2E6YXNzZXRzOnBkZjoyMDIyLWFyeGl2LXNoYW5haGFuLXRhbGtpbmctYW5ub3RhdGVkMjAyMzA2MTIucGRmAA4AZAAxADIAMAAyADIALQBhAHIAeABpAHYALQBzAGgAYQBuAGEAaABhAG4ALQB0AGEAbABrAGkAbgBnAC0AYQBuAG4AbwB0AGEAdABlAGQAMgAwADIAMwAwADYAMQAyAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBgVXNlcnMvbWNyb3dsZXkvcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIyLWFyeGl2LXNoYW5haGFuLXRhbGtpbmctYW5ub3RhdGVkMjAyMzA2MTIucGRmABMAAS8AABUAAgAP//8AAAAIAA0AGgAkAIQAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAChg==},
	bdsk-url-1 = {https://arxiv.org/pdf/2212.03551.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/2212.03551}}

@article{lewis2019arxiv,
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	date-added = {2023-05-28 19:30:54 -0400},
	date-modified = {2023-09-06 17:33:57 -0400},
	in-website = {1},
	journal = {arXiv preprint arXiv:1910.13461},
	keywords = {done},
	note = {This paper builds on the success of BERT but maintaining full encoder-decoder framework.},
	order = {5},
	pdf = {2019-arxiv-lewis-bart},
	title = {Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
	toread = {1},
	venue-short = {arxiv},
	year = {2019},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBFLi4vLi4vLi4vLi4vcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDE5LWFyeGl2LWxld2lzLWJhcnQucGRmTxEBngAAAAABngACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////GTIwMTktYXJ4aXYtbGV3aXMtYmFydC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAABAAFAAAKIGN1AAAAAAAAAAAAAAAAAANwZGYAAAIASi86VXNlcnM6bWNyb3dsZXk6cmVwb3M6bWFya2Nyb3dsZXktY2E6YXNzZXRzOnBkZjoyMDE5LWFyeGl2LWxld2lzLWJhcnQucGRmAA4ANAAZADIAMAAxADkALQBhAHIAeABpAHYALQBsAGUAdwBpAHMALQBiAGEAcgB0AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBIVXNlcnMvbWNyb3dsZXkvcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDE5LWFyeGl2LWxld2lzLWJhcnQucGRmABMAAS8AABUAAgAP//8AAAAIAA0AGgAkAGwAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAACDg==}}

@article{taylor2022arxiv,
	archiveprefix = {arXiv},
	author = {Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and Anthony Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},
	date-added = {2023-05-28 19:21:44 -0400},
	date-modified = {2023-09-06 17:33:31 -0400},
	eprint = {2211.09085},
	in-website = {1},
	journal = {Arxiv Preprint},
	keywords = {potential},
	note = {A promising approach to scientific reasoning with LLMs.},
	order = {?},
	pdf = {2022-arxiv-taylor-galactica},
	primaryclass = {cs.CL},
	title = {Galactica: A Large Language Model for Science},
	toread = {1},
	venue-short = {arxiv},
	year = {2022},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBLLi4vLi4vLi4vLi4vcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIyLWFyeGl2LXRheWxvci1nYWxhY3RpY2EucGRmTxEBtgAAAAABtgACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////HzIwMjItYXJ4aXYtdGF5bG9yLWdhbGFjdGljYS5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAABAAFAAAKIGN1AAAAAAAAAAAAAAAAAANwZGYAAAIAUC86VXNlcnM6bWNyb3dsZXk6cmVwb3M6bWFya2Nyb3dsZXktY2E6YXNzZXRzOnBkZjoyMDIyLWFyeGl2LXRheWxvci1nYWxhY3RpY2EucGRmAA4AQAAfADIAMAAyADIALQBhAHIAeABpAHYALQB0AGEAeQBsAG8AcgAtAGcAYQBsAGEAYwB0AGkAYwBhAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBOVXNlcnMvbWNyb3dsZXkvcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIyLWFyeGl2LXRheWxvci1nYWxhY3RpY2EucGRmABMAAS8AABUAAgAP//8AAAAIAA0AGgAkAHIAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAACLA==}}

@article{radfordopenai,
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	date-added = {2023-05-28 19:19:44 -0400},
	date-modified = {2023-09-06 17:33:43 -0400},
	in-website = {1},
	journal = {Preprint},
	keywords = {transformers, gpt, openai, nlp, large-language-models; done},
	local-url = {2018-openai-radford-improving},
	note = {The founding paper for GPT 1.0 posted online as a preprint. (File: 2018-openai-radford-improving)},
	order = {3},
	paperdesc = {The original paper for the GPT 1.0 model.},
	pdf = {2018-openai-radford-improving},
	read = {0},
	title = {Improving Language Understanding by Generative Pre-Training},
	toread = {0},
	venue-short = {openai},
	year = {2018},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBNLi4vLi4vLi4vLi4vcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDE4LW9wZW5haS1yYWRmb3JkLWltcHJvdmluZy5wZGZPEQG+AAAAAAG+AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fMjAxOC1vcGVuYWktcmFkZm9yI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAAEAAUAAAogY3UAAAAAAAAAAAAAAAAAA3BkZgAAAgBSLzpVc2VyczptY3Jvd2xleTpyZXBvczptYXJrY3Jvd2xleS1jYTphc3NldHM6cGRmOjIwMTgtb3BlbmFpLXJhZGZvcmQtaW1wcm92aW5nLnBkZgAOAEQAIQAyADAAMQA4AC0AbwBwAGUAbgBhAGkALQByAGEAZABmAG8AcgBkAC0AaQBtAHAAcgBvAHYAaQBuAGcALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAFBVc2Vycy9tY3Jvd2xleS9yZXBvcy9tYXJrY3Jvd2xleS1jYS9hc3NldHMvcGRmLzIwMTgtb3BlbmFpLXJhZGZvcmQtaW1wcm92aW5nLnBkZgATAAEvAAAVAAIAD///AAAACAANABoAJAB0AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAjY=},
	bdsk-file-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBXLi4vLi4vLi4vLi4vcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDE4LW9wZW5haS1yYWRmb3JkLWltcHJvdmluZy1hbm5vdGF0ZWQucGRmTxEB5gAAAAAB5gACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////HzIwMTgtb3BlbmFpLXJhZGZvciNGRkZGRkZGRi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAABAAFAAAKIGN1AAAAAAAAAAAAAAAAAANwZGYAAAIAXC86VXNlcnM6bWNyb3dsZXk6cmVwb3M6bWFya2Nyb3dsZXktY2E6YXNzZXRzOnBkZjoyMDE4LW9wZW5haS1yYWRmb3JkLWltcHJvdmluZy1hbm5vdGF0ZWQucGRmAA4AWAArADIAMAAxADgALQBvAHAAZQBuAGEAaQAtAHIAYQBkAGYAbwByAGQALQBpAG0AcAByAG8AdgBpAG4AZwAtAGEAbgBuAG8AdABhAHQAZQBkAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBaVXNlcnMvbWNyb3dsZXkvcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDE4LW9wZW5haS1yYWRmb3JkLWltcHJvdmluZy1hbm5vdGF0ZWQucGRmABMAAS8AABUAAgAP//8AAAAIAA0AGgAkAH4AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAACaA==},
	bdsk-file-3 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBeLi4vLi4vLi4vLi4vcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDE4LW9wZW5haS1yYWRmb3JkLWltcHJvdmluZy1hbm5vdGF0ZWQtZXhwb3J0LnBkZk8RAgQAAAAAAgQAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x8yMDE4LW9wZW5haS1yYWRmb3IjRkZGRkZGRkYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAQABQAACiBjdQAAAAAAAAAAAAAAAAADcGRmAAACAGMvOlVzZXJzOm1jcm93bGV5OnJlcG9zOm1hcmtjcm93bGV5LWNhOmFzc2V0czpwZGY6MjAxOC1vcGVuYWktcmFkZm9yZC1pbXByb3ZpbmctYW5ub3RhdGVkLWV4cG9ydC5wZGYAAA4AZgAyADIAMAAxADgALQBvAHAAZQBuAGEAaQAtAHIAYQBkAGYAbwByAGQALQBpAG0AcAByAG8AdgBpAG4AZwAtAGEAbgBuAG8AdABhAHQAZQBkAC0AZQB4AHAAbwByAHQALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAGFVc2Vycy9tY3Jvd2xleS9yZXBvcy9tYXJrY3Jvd2xleS1jYS9hc3NldHMvcGRmLzIwMTgtb3BlbmFpLXJhZGZvcmQtaW1wcm92aW5nLWFubm90YXRlZC1leHBvcnQucGRmAAATAAEvAAAVAAIAD///AAAACAANABoAJACFAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAo0=}}

@article{thoppilan2022arxiv,
	archiveprefix = {arXiv},
	author = {Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and YaGuang Li and Hongrae Lee and Huaixiu Steven Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and Maxim Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Vincent Zhao and Yanqi Zhou and Chung-Ching Chang and Igor Krivokon and Will Rusch and Marc Pickett and Pranesh Srinivasan and Laichee Man and Kathleen Meier-Hellstern and Meredith Ringel Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and Johnny Soraker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark Diaz and Ben Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravi Rajakumar and Alena Butryna and Matthew Lamm and Viktoriya Kuzmina and Joe Fenton and Aaron Cohen and Rachel Bernstein and Ray Kurzweil and Blaise Aguera-Arcas and Claire Cui and Marian Croak and Ed Chi and Quoc Le},
	date-added = {2023-05-28 19:13:45 -0400},
	date-modified = {2023-09-06 17:34:07 -0400},
	eprint = {2201.08239},
	in-website = {1},
	journal = {Arxiv Preprint},
	keywords = {done},
	order = {6},
	pdf = {2022-arxiv-thoppilan-lamda},
	primaryclass = {cs.CL},
	title = {LaMDA: Language Models for Dialog Applications},
	toread = {1},
	venue-short = {arxiv},
	year = {2022},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBKLi4vLi4vLi4vLi4vcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIyLWFyeGl2LXRob3BwaWxhbi1sYW1kYS5wZGZPEQG0AAAAAAG0AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8eMjAyMi1hcnhpdi10aG9wcGlsYW4tbGFtZGEucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAAEAAUAAAogY3UAAAAAAAAAAAAAAAAAA3BkZgAAAgBPLzpVc2VyczptY3Jvd2xleTpyZXBvczptYXJrY3Jvd2xleS1jYTphc3NldHM6cGRmOjIwMjItYXJ4aXYtdGhvcHBpbGFuLWxhbWRhLnBkZgAADgA+AB4AMgAwADIAMgAtAGEAcgB4AGkAdgAtAHQAaABvAHAAcABpAGwAYQBuAC0AbABhAG0AZABhAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBNVXNlcnMvbWNyb3dsZXkvcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIyLWFyeGl2LXRob3BwaWxhbi1sYW1kYS5wZGYAABMAAS8AABUAAgAP//8AAAAIAA0AGgAkAHEAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAACKQ==}}

@article{yang2023arxiv,
	archiveprefix = {arXiv},
	author = {Jingfeng Yang and Hongye Jin and Ruixiang Tang and Xiaotian Han and Qizhang Feng and Haoming Jiang and Bing Yin and Xia Hu},
	date-added = {2023-05-28 18:56:45 -0400},
	date-modified = {2023-09-06 17:34:41 -0400},
	eprint = {2304.13712},
	in-website = {1},
	journal = {Arxiv Preprint},
	keywords = {upcoming},
	note = {I'm wary of some of their intro, "BERT models started to disappear" it's only been a year or two. They have a very nice overview figure. This recent review paper gives content on what tasks GPT style decoder-only LLMs are good for and which they are not (most tasks in fact).},
	order = 8,
	pdf = {https://www.dropbox.com/scl/fi/mwskgcan1he9txith56v8/2023-nature-feng-dense.pdf?rlkey=xkejcymfg14n6l8i2rgln6c3r&raw=1},
	primaryclass = {cs.CL},
	title = {Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond},
	toread = {1},
	venue-short = {arxiv},
	year = {2023},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBKLi4vLi4vLi4vLi4vcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIzLWFyeGl2LXlhbmctaGFybmVzc2luZy5wZGZPEQG0AAAAAAG0AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8eMjAyMy1hcnhpdi15YW5nLWhhcm5lc3NpbmcucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAAEAAUAAAogY3UAAAAAAAAAAAAAAAAAA3BkZgAAAgBPLzpVc2VyczptY3Jvd2xleTpyZXBvczptYXJrY3Jvd2xleS1jYTphc3NldHM6cGRmOjIwMjMtYXJ4aXYteWFuZy1oYXJuZXNzaW5nLnBkZgAADgA+AB4AMgAwADIAMwAtAGEAcgB4AGkAdgAtAHkAYQBuAGcALQBoAGEAcgBuAGUAcwBzAGkAbgBnAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBNVXNlcnMvbWNyb3dsZXkvcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIzLWFyeGl2LXlhbmctaGFybmVzc2luZy5wZGYAABMAAS8AABUAAgAP//8AAAAIAA0AGgAkAHEAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAACKQ==}}

@inproceedings{devlin2019naaclhlt,
	author = {Jacob Devlin and Ming-Wei Chang and Kenten Lee and Kristina Toutanova},
	booktitle = {Proceedings of NAACL-HLT},
	date-added = {2023-05-28 15:13:21 -0400},
	date-modified = {2023-07-04 12:54:34 -0400},
	in-website = {1},
	keywords = {potential, machine-learning, transformers, nlp, BERT, large-language-models},
	note = {The original paper for the BERT transformer model for NLP tasks.},
	order = {3},
	pages = {4171--4186},
	paperdesc = {The original paper for the BERT transformer model for NLP tasks.},
	pdf = {2019-naaclhlt-kenton-bert pre-training of deep bidirectional transformers for language understanding},
	status = {1},
	title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	toread = {1},
	venue-short = {naaclhlt},
	year = {2019},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBJLi4vLi4vLi4vLi4vcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDE5LW5hYWNsaGx0LWRldmxpbi1iZXJ0LnBkZk8RAa4AAAAAAa4AAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x0yMDE5LW5hYWNsaGx0LWRldmxpbi1iZXJ0LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAQABQAACiBjdQAAAAAAAAAAAAAAAAADcGRmAAACAE4vOlVzZXJzOm1jcm93bGV5OnJlcG9zOm1hcmtjcm93bGV5LWNhOmFzc2V0czpwZGY6MjAxOS1uYWFjbGhsdC1kZXZsaW4tYmVydC5wZGYADgA8AB0AMgAwADEAOQAtAG4AYQBhAGMAbABoAGwAdAAtAGQAZQB2AGwAaQBuAC0AYgBlAHIAdAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIATFVzZXJzL21jcm93bGV5L3JlcG9zL21hcmtjcm93bGV5LWNhL2Fzc2V0cy9wZGYvMjAxOS1uYWFjbGhsdC1kZXZsaW4tYmVydC5wZGYAEwABLwAAFQACAA///wAAAAgADQAaACQAcAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAIi}}

@article{liu2019,
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	annote = {Where does the annotation show up? anywhere?},
	author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
	date-added = {2023-05-28 15:09:08 -0400},
	date-modified = {2023-06-20 21:34:03 -0400},
	eprint = {1907.11692},
	hypothesis = {https://hyp.is/go?url=https%3A%2F%2Farxiv.org%2Fpdf%2F1907.11692.pdf&group=__world__},
	in-website = {1},
	keywords = {potential, transformers, BERT, large-language-models},
	month = {07},
	order = {4},
	pdf = {2019-arxiv-liu-roberta.pdf},
	seminal = {1},
	status = {1},
	title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
	toread = {1},
	url = {https://arxiv.org/pdf/1907.11692.pdf},
	venue-short = {arxiv},
	year = {2019},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBGLi4vLi4vLi4vLi4vcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDE5LWFyeGl2LWxpdS1yb2JlcnRhLnBkZk8RAaQAAAAAAaQAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////xoyMDE5LWFyeGl2LWxpdS1yb2JlcnRhLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAQABQAACiBjdQAAAAAAAAAAAAAAAAADcGRmAAACAEsvOlVzZXJzOm1jcm93bGV5OnJlcG9zOm1hcmtjcm93bGV5LWNhOmFzc2V0czpwZGY6MjAxOS1hcnhpdi1saXUtcm9iZXJ0YS5wZGYAAA4ANgAaADIAMAAxADkALQBhAHIAeABpAHYALQBsAGkAdQAtAHIAbwBiAGUAcgB0AGEALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAElVc2Vycy9tY3Jvd2xleS9yZXBvcy9tYXJrY3Jvd2xleS1jYS9hc3NldHMvcGRmLzIwMTktYXJ4aXYtbGl1LXJvYmVydGEucGRmAAATAAEvAAAVAAIAD///AAAACAANABoAJABtAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAhU=},
	bdsk-url-1 = {https://arxiv.org/pdf/1907.11692.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1907.11692},
	bdsk-url-3 = {https://openreview.net/forum?id=SyxS0T4tvS}}

@inproceedings{chen2021neurips,
	abstract = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
	archiveprefix = {arxiv},
	author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2023-05-28 14:42:06 -0400},
	date-modified = {2023-07-04 13:02:15 -0400},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	eprint = {2106.01345},
	howpublished = {arxiv},
	in-website = {1},
	keywords = {upcoming, machine-learning, transformers, reinforcement-learning, decision-transformers},
	month = jun,
	number = {arXiv:2106.01345},
	order = {9?},
	pages = {15084--15097},
	primaryclass = {cs},
	shorttitle = {Decision {{Transformer}}},
	status = {1},
	title = {Decision Transformer: Reinforcement Learning via Sequence Modeling},
	toread = {1},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/7f489f642a0ddb10272b5c31057f0663-Paper.pdf},
	urldate = {2022-09-27},
	venue-short = {neurips},
	volume = {34},
	year = {2021},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBKLi4vLi4vLi4vLi4vcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIxLW5ldXJpcHMtY2hlbi1kZWNpc2lvbi5wZGZPEQG0AAAAAAG0AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8eMjAyMS1uZXVyaXBzLWNoZW4tZGVjaXNpb24ucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAAEAAUAAAogY3UAAAAAAAAAAAAAAAAAA3BkZgAAAgBPLzpVc2VyczptY3Jvd2xleTpyZXBvczptYXJrY3Jvd2xleS1jYTphc3NldHM6cGRmOjIwMjEtbmV1cmlwcy1jaGVuLWRlY2lzaW9uLnBkZgAADgA+AB4AMgAwADIAMQAtAG4AZQB1AHIAaQBwAHMALQBjAGgAZQBuAC0AZABlAGMAaQBzAGkAbwBuAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBNVXNlcnMvbWNyb3dsZXkvcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIxLW5ldXJpcHMtY2hlbi1kZWNpc2lvbi5wZGYAABMAAS8AABUAAgAP//8AAAAIAA0AGgAkAHEAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAACKQ==},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/7f489f642a0ddb10272b5c31057f0663-Paper.pdf}}

@inproceedings{mahendran2022acmwww,
	abstract = {Extracting information regarding novel chemicals and chemical reactions from chemical patents plays a vital role in the chemical and pharmaceutical industry. Due to the increasing volume of chemical patents, there is an urgent need for automated solutions to extract relations between chemical compounds. Several studies have used models that apply attention mechanisms such as Bidirectional Encoder Representations from Transformers (BERT) to capture the contextual information within a text. However, these models do not capture the global information about a specific vocabulary. On the other hand, Graph Convolutional Networks (GCNs) capture global dependencies between terms within a corpus but not the local contextual information. In this work, we propose two novel approaches, GCN-Vanilla and GCN-BERT, for chemical relation extraction. GCN-Vanilla approach builds a single graph for the whole corpus based on word co-occurrence and sentence-word relations. Then, we model the graph with GCN to capture the global information and classify the sentence nodes. GCN-BERT approach combines GCN and BERT to capture both global and local information, and build together a final representation for relation extraction. We evaluate our approaches on the CLEF-2020 dataset. Our results show the combined GCN-BERT approach outperforms standalone BERT and GCN models, and achieves a higher F1 than that reported in our previous studies.},
	address = {{New York, NY, USA}},
	author = {Mahendran, Darshini and Tang, Christina and McInnes, Bridget T.},
	booktitle = {Companion {{Proceedings}} of the {{Web Conference}} 2022},
	date-added = {2023-05-28 14:42:06 -0400},
	date-modified = {2023-06-14 16:54:20 -0400},
	doi = {10.1145/3487553.3524702},
	in-website = {1},
	isbn = {978-1-4503-9130-6},
	keywords = {BERT,chemical natural language processing,graph convolutional neural networks,relation extraction; potential},
	month = apr,
	pages = {833--842},
	publisher = {{Association for Computing Machinery}},
	series = {{{WWW}} '22},
	status = {1},
	title = {Graph {{Convolutional Networks}} for {{Chemical Relation Extraction}}},
	toread = {1},
	urldate = {2022-09-26},
	venue-short = {ACMWWW},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3487553.3524702}}

@inproceedings{theile2020ieeersj,
	annotation = {IROS},
	author = {Theile, Mirco and Bayerlein, Harald and Nai, Richard and Gesbert, David and Caccamo, Marco},
	booktitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
	date-added = {2023-05-28 14:42:06 -0400},
	date-modified = {2023-06-14 16:54:23 -0400},
	in-website = {1},
	keywords = {potential},
	pages = {1444--1449},
	publisher = {{IEEE}},
	status = {1},
	title = {{{UAV}} Coverage Path Planning under Varying Power Constraints Using Deep Reinforcement Learning},
	toread = {1},
	venue-short = {IEEERSJ},
	year = {2020}}

@inproceedings{yang2020acmikm,
	address = {{Virtual Event Ireland}},
	annotation = {ACM-IKM},
	author = {Yang, Liu and Zhang, Mingyang and Li, Cheng and Bendersky, Michael and Najork, Marc},
	booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
	date-added = {2023-05-28 14:42:06 -0400},
	date-modified = {2023-06-14 16:54:23 -0400},
	doi = {10.1145/3340531.3411908},
	in-website = {1},
	isbn = {978-1-4503-6859-9},
	keywords = {NLP,siamese,transformers; potential},
	langid = {english},
	month = oct,
	pages = {1725--1734},
	publisher = {{ACM}},
	status = {1},
	title = {Beyond 512 {{Tokens}}: {{Siamese Multi-depth Transformer-based Hierarchical Encoder}} for {{Long-Form Document Matching}}},
	toread = {1},
	urldate = {2021-02-25},
	venue-short = {acmikm},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3340531.3411908}}

@inproceedings{igl2022icra,
	author = {Igl, Maximilian and Kim, Daewoo and Kuefler, Alex and Mougin, Paul and Shah, Punit and Shiarlis, Kyriacos and Anguelov, Dragomir and Palatucci, Mark and White, Brandyn and Whiteson, Shimon},
	booktitle = {International Conference on Robotics and Automation (ICRA)},
	date-added = {2023-05-26 21:51:52 -0400},
	date-modified = {2023-06-14 16:54:21 -0400},
	doi = {https://ieeexplore.ieee.org/document/9811990},
	in-website = {1},
	keywords = {autonomous-driving, machine-learning; potential},
	pages = {2445-2451},
	status = {1},
	title = {Symphony: Learning Realistic and Diverse Agents for Autonomous Driving Simulation},
	toread = {1},
	venue-short = {ICRA},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/ICRA46639.2022.9811990},
	bdsk-url-2 = {https://arxiv.org/abs/2205.03195}}

@unpublished{ghojogh2020osfpreprint,
	abstract = {This is a tutorial and survey paper on the attention mechanism, transformers, BERT, and GPT. We first explain attention mechanism, sequence-to-sequence model without and with attention, self-attention, and attention in different areas such as natural language processing and computer vision. Then, we explain transformers which do not use any recurrence. We explain all the parts of encoder and decoder in the transformer, including positional encoding, multihead self-attention and cross-attention, and masked multihead attention. Thereafter, we introduce the Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT) as the stacks of encoders and decoders of transformer, respectively. We explain their characteristics and how they work.},
	author = {Benyamin Ghojogh and Ali Ghodsi},
	cited-by = {https://paperswithcode.com/paper/attention-mechanism-transformers-bert-and-gpt},
	date-added = {2023-05-24 20:45:02 -0400},
	date-modified = {2023-07-04 12:54:34 -0400},
	doi = {https://osf.io/m6gcn},
	howpublished = {preprint},
	in-website = {1},
	keywords = {done,machine-learning, transformer, nlp, recurrent-neural-networks, lstm},
	month = {December},
	order = {1},
	pdf = {2020-osfpreprin-ghojogh-attention},
	status = {1},
	title = {Attention Mechanism, Transformers, BERT, and GPT: Tutorial and Survey},
	toread = {1},
	url = {https://osf.io/m6gcn},
	venue-short = {osfpreprint},
	year = {2020},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBRLi4vLi4vLi4vLi4vcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIwLW9zZnByZXByaW4tZ2hvam9naC1hdHRlbnRpb24ucGRmTxEBzgAAAAABzgACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////HzIwMjAtb3NmcHJlcHJpbi1naCNGRkZGRkZGRi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAABAAFAAAKIGN1AAAAAAAAAAAAAAAAAANwZGYAAAIAVi86VXNlcnM6bWNyb3dsZXk6cmVwb3M6bWFya2Nyb3dsZXktY2E6YXNzZXRzOnBkZjoyMDIwLW9zZnByZXByaW4tZ2hvam9naC1hdHRlbnRpb24ucGRmAA4ATAAlADIAMAAyADAALQBvAHMAZgBwAHIAZQBwAHIAaQBuAC0AZwBoAG8AagBvAGcAaAAtAGEAdAB0AGUAbgB0AGkAbwBuAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBUVXNlcnMvbWNyb3dsZXkvcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDIwLW9zZnByZXByaW4tZ2hvam9naC1hdHRlbnRpb24ucGRmABMAAS8AABUAAgAP//8AAAAIAA0AGgAkAHgAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAACSg==}}

@inproceedings{vaswani2017neurips,
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	address = {Long Beach California, USA},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2023-05-24 20:37:40 -0400},
	date-modified = {2023-06-14 16:54:12 -0400},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	in-website = {1},
	keywords = {done, machine-learning, transformer, recurrent-neural-networks, lstm,nlp},
	month = {December},
	order = {2},
    date-discussed = {2023-05-01},
	pdf = {2017-neurips-vaswani-attention},
	seminal = {1},
	status = {1},
	title = {Attention is All you Need},
	toread = {0},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	venue-short = {neurips},
	volume = {30},
	year = {2017},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBPLi4vLi4vLi4vLi4vcmVwb3MvbWFya2Nyb3dsZXktY2EvYXNzZXRzL3BkZi8yMDE3LW5ldXJpcHMtdmFzd2FuaS1hdHRlbnRpb24xLnBkZk8RAcYAAAAAAcYAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x8yMDE3LW5ldXJpcHMtdmFzd2EjRkZGRkZGRkYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAQABQAACiBjdQAAAAAAAAAAAAAAAAADcGRmAAACAFQvOlVzZXJzOm1jcm93bGV5OnJlcG9zOm1hcmtjcm93bGV5LWNhOmFzc2V0czpwZGY6MjAxNy1uZXVyaXBzLXZhc3dhbmktYXR0ZW50aW9uMS5wZGYADgBIACMAMgAwADEANwAtAG4AZQB1AHIAaQBwAHMALQB2AGEAcwB3AGEAbgBpAC0AYQB0AHQAZQBuAHQAaQBvAG4AMQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAUlVzZXJzL21jcm93bGV5L3JlcG9zL21hcmtjcm93bGV5LWNhL2Fzc2V0cy9wZGYvMjAxNy1uZXVyaXBzLXZhc3dhbmktYXR0ZW50aW9uMS5wZGYAEwABLwAAFQACAA///wAAAAgADQAaACQAdgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAJA},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}}
