---
layout: page
title: Automated Materials Design and Discovery Using Reinforcement Learning
cardtitle: Digital Chemistry
name: ChemGymRL
bibkeyword: proj-chemgymrl
keywords: machine-learning
permalink: /chemgymrl/
status: active
domains: ai-for-material-design, digital-chemistry, ai-for-physics, ai-for-science, ai-for-chemistry
methods: reinforcement-learning, MARL, MDP
description: Studying how to automate material synthesis and discovery by training a Deep Reinforcement Learning system to plan and carry out chemical synthesis experiments to gather data and find efficient pathways to making new or known materials.
oneline: Using AI to learn how to be a chemist.
publish: true
people: nouhachatti,zihanyang, sriramganapathisubramanian, isaactamblyn, markcrowley, colinbellinger
stage: project
showmethods: true
showwebpage: true
webpage: http://chemgymrl.com
importance: 1
img: /assets/pdf/2022-canai-bellinger-balancing.png
---
In early 2019 the lab began a new collaboration sponsored by the **National Research Council â€“ UW Collaboration Centre (NUCC) on AI/Cybersecurity/IoT**. This is a new organization set-up to initial research collaboration between NRC staff researchers and UW PIs. I am one of the first faculty to be a part of this endeavour and to receive funding for my work. With Dr. Isaac Tamblyn (NRC) and Dr. Colin Bellinger (NRC), our project studies how to automate material synthesis and discovery by training a [Deep Reinforcement Learning](/reinforcement-learning/) system to plan and carry out chemical synthesis experiments to gather data and find efficient pathways to making new or known materials.

| <img src="/assets/img/chemgymrl/chemgymrl_logo_400x400.png" width=100> | To find out more, take a look at the current framework to carry out your own experiments or contribute to the framework: <a href="https://chemgymrl.com/">chemgymrl.com</a> |


<hr>
## Problem Description

The main idea is described most recently in our paper {%cite Bellinger2022Balancing %} with some great high-level summary and motivation. This paper also outlines a modification to the standard single-action RL framework, to divide actions into two parts containing an costly observation choice in addition to the usual action which affects the world. 



