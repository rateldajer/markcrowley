<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Mark  Crowley | Sustainable Forest Management</title>
<meta name="description" content="">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/forest-management/">

<!-- Theming-->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Mark</span>   Crowley
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/bio/">
                bio
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/contact/">
                contact
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/lab/">
                lab
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/news/">
                news
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/showcase/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/topics/">
                topics
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
      
          
            <h1 class="post-title">Sustainable Forest Management</h1>
        
      
    <p class="post-description" style="border-bottom-style:dashed; border-bottom-color:lightgrey; border-bottom-width:1px; margin:1px; margin-bottom:10px;font-style: italic;">All research related to assessment, modelling, prediction and planning for the sustainable management of forest ecosystems and resources.</p>

      
        
        <p class="post-description" style="border-bottom-style:solid; border-bottom-color:lightgrey; border-bottom-width:3px; margin:1px;">
        <b>DOMAINS</b>
        
            
            | <a href="/forest-management/">forest-management</a> 
        
            
            | <a href="/forest-wildfire/">forest-wildfire</a> 
        
            
            | <a href="/computational-sustainability/">computational-sustainability</a> 
        
        </p>
        
      
      
      
      
      
        <p></p>
  </header>

  <article>
      
    <p>The broad domain of <a href="/forest-management/">Sustainable Forest Management</a>, includes a wide variety of tasks and challenges. Prof. Crowley’s PhD research focussed on this domain from a harvesting pointing of view utilizing probabilistic modelling, simulation, and reinforcement learning. More recently, the lab carries out some research on the task of <a href="/forest-wildfire">Forest Wildfire Management</a> which presents a number of unique challenges which push the boundaries of what is possible with existing AI/ML algorithms.</p>


  </article>


      
          
            <div class="publications">
              <h2>Our Papers on Sustainable Forest Management</h2> 
            <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Multi-Advisor-MARL</abbr>
    
  
  </div>

  <div id="ganapathi-subramanian2023aamas" class="col-sm-8">
      <div class="title">
          
          Learning from Multiple Independent Advisors in Multi-agent Reinforcement Learning
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://sriramsubramanian.com/" target="_blank">Sriram Ganapathi Subramanian</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://irll.ca" target="_blank">Matthew Taylor</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://cs.uwaterloo.ca/~klarson/" target="_blank">Kate Larson</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>Proceedings of the 22nd International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)</em>. 

      

      
          International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS),
      
      
          London, United Kingdom.
      
      
          Sep,
      
      
        2023.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2023-aamas-ganapathi%20subramanian-learning%20from%20multiple%20independent%20advisors%20in%20multi-agent%20reinforcement%20learning.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         Multi-agent reinforcement learning typically suffers from the problem of sample inefficiency, where learning suitable policies involves the use of many data samples. Learning from external demonstrators is a possible solution that mitigates this problem. However, most prior approaches in this area assume the presence of a single demonstrator. Leveraging multiple knowledge sources (i.e., advisors) with expertise in distinct aspects of the environment could substantially speed up learning in complex environments. This paper considers the problem of simultaneously learning from multiple independent advisors in multi-agent reinforcement learning. The approach leverages a two-level Q-learning architecture, and extends this framework from single-agent to multi-agent settings. We provide principled algorithms that incorporate a set of advisors by both evaluating the advisors at each state and subsequently using the advisors to guide action selection. Also, we provide theoretical convergence and sample complexity guarantees. Experimentally, we validate our approach in three different test-beds and show that our algorithms give better performances than baselines, can effectively integrate the combined expertise of different advisors, and learn to ignore bad advice.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Multi-Advisor-QL</abbr>
    
  
  </div>

  <div id="ganapathisubramanian2023ijcai" class="col-sm-8">
      <div class="title">
          
          Multi-Agent Advisor Q-Learning
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://sriramsubramanian.com/" target="_blank">Sriram Ganapathi Subramanian</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://cs.uwaterloo.ca/~klarson/" target="_blank">Kate Larson</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://irll.ca" target="_blank">Matthew Taylor</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>International Joint Conference on Artificial Intelligence (IJCAI) : Journal Track</em>. 

      

      
      
          Macao, China.
      
      
          Aug,
      
      
        2023.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2022-jair-ganapathi%20subramanian-multi-agent.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/Sriram94/multiagentadvisorqlearning" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
      <a href="https://doi.org/10.1613/jair.1.13445" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
      <a href="https://recorder-v3.slideslive.com/?share=82208&s=5fe77823-b4c3-4f27-99ba-b59e71f4a7c4" class="btn btn-sm z-depth-0" role="button" target="_blank">Video</a>
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         In the last decade, there have been significant advances in multi-agent reinforcement learn- ing (MARL) but there are still numerous challenges, such as high sample complexity and slow convergence to stable policies, that need to be overcome before wide-spread deployment is possi- ble. However, many real-world environments already, in practice, deploy sub-optimal or heuristic approaches for generating policies. An interesting question that arises is how to best use such approaches as advisors to help improve reinforcement learning in multi-agent domains. In this paper, we provide a principled framework for incorporating action recommendations from online sub- optimal advisors in multi-agent settings. We describe the problem of ADvising Multiple Intelligent Reinforcement Agents (ADMIRAL) in nonrestrictive general-sum stochastic game environments and present two novel Q-learning based algorithms: ADMIRAL - Decision Making (ADMIRAL-DM) and ADMIRAL - Advisor Evaluation (ADMIRAL-AE), which allow us to improve learning by appropriately incorporating advice from an advisor (ADMIRAL-DM), and evaluate the effectiveness of an advisor (ADMIRAL-AE). We analyze the algorithms theoretically and provide fixed point guarantees regarding their learning in general-sum stochastic games. Furthermore, extensive experi- ments illustrate that these algorithms: can be used in a variety of environments, have performances that compare favourably to other related baselines, can scale to large state-action spaces, and are robust to poor advice from advisors.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">MARLEmpircal</abbr>
    
  
  </div>

  <div id="lee2021frontai" class="col-sm-8">
      <div class="title">
          
          Investigation of Independent Reinforcement Learning Algorithms in Multi-Agent Environments
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://www.linkedin.com/in/km-lee/" target="_blank">Ken Ming Lee</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://sriramsubramanian.com/" target="_blank">Sriram Ganapathi Subramanian</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
        <em>Frontiers in Artificial Intelligence</em>.

          
          

      

      
      
      
          Sep,
      
      
        2022.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2022-frontai-lee-investigation.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://www.frontiersin.org/articles/10.3389/frai.2022.805823" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         Independent reinforcement learning algorithms have no theoretical guarantees for finding the best policy in multi-agent settings. However, in practice, prior works have reported good performance with independent algorithms in some domains and bad performance in others. Moreover, a comprehensive study of the strengths and weaknesses of independent algorithms is lacking in the literature. In this paper, we carry out an empirical comparison of the performance of independent algorithms on seven PettingZoo environments that span the three main categories of multi-agent environments, i.e., cooperative, competitive, and mixed. For the cooperative setting, we show that independent algorithms can perform on par with multi-agent algorithms in fully-observable environments, while adding recurrence improves the learning of independent algorithms in partially-observable environments. In the competitive setting, independent algorithms can perform on par or better than multi-agent algorithms, even in more challenging environments. We also show that agents trained via independent algorithms learn to perform well individually, but fail to learn to cooperate with allies and compete with enemies in mixed environments.

</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Multi-Advisor-QL</abbr>
    
  
  </div>

  <div id="ganapathisubramanian2022jair" class="col-sm-8">
      <div class="title">
          
          Multi-Agent Advisor Q-Learning
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://sriramsubramanian.com/" target="_blank">Sriram Ganapathi Subramanian</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://cs.uwaterloo.ca/~klarson/" target="_blank">Kate Larson</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://irll.ca" target="_blank">Matthew Taylor</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
        <em>Journal of Artificial Intelligence Research (JAIR)</em>.

          
              74,
          
          

      

      
      
      
          May,
      
      
        2022.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2022-jair-ganapathi%20subramanian-multi-agent.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/Sriram94/multiagentadvisorqlearning" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
      <a href="https://doi.org/10.1613/jair.1.13445" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         In the last decade, there have been significant advances in multi-agent reinforcement learning (MARL) but there are still numerous challenges, such as high sample complexity and slow convergence to stable policies, that need to be overcome before wide-spread deployment is possible. However, many real-world environments already, in practice, deploy sub-optimal or heuristic approaches for generating policies. An interesting question that arises is how to best use such approaches as advisors to help improve reinforcement learning in multi-agent domains. In this paper, we provide a principled framework for incorporating action recommendations from online sub-optimal advisors in multi-agent settings. We describe the problem of ADvising Multiple Intelligent Reinforcement Agents (ADMIRAL) in nonrestrictive general-sum stochastic game environments and present two novel Q-learning based algorithms: ADMIRAL - Decision Making (ADMIRAL-DM) and ADMIRAL - Advisor Evaluation (ADMIRAL-AE), which allow us to improve learning by appropriately incorporating advice from an advisor (ADMIRAL-DM), and evaluate the effectiveness of an advisor (ADMIRAL-AE). We analyze the algorithms theoretically and provide fixed point guarantees regarding their learning in general-sum stochastic games. Furthermore, extensive experi- ments illustrate that these algorithms: can be used in a variety of environments, have performances that compare favourably to other related baselines, can scale to large state-action spaces, and are robust to poor advice from advisors.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">PO-MFRL</abbr>
    
  
  </div>

  <div id="ganapathisubramanian2021aamas" class="col-sm-8">
      <div class="title">
          
          Partially Observable Mean Field Reinforcement Learning
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://sriramsubramanian.com/" target="_blank">Sriram Ganapathi Subramanian</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://irll.ca" target="_blank">Matthew Taylor</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    <em>Mark Crowley</em>,
                  
                
              
        
            
            
              
                
                  
                    
                      and Pascal Poupart.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)</em>. 

      

      
          International Foundation for Autonomous Agents and Multiagent Systems,
      
      
          London, United Kingdom.
      
      
          May,
      
      
        2021.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2012.15791" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
      
      <a href="/assets/pdf/2021-aamas-ganapathi%20subramanian-partially.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         Traditional multi-agent reinforcement learning algorithms are not scalable to environments with more than a few agents, since these algorithms are exponential in the number of agents. Recent research has introduced successful methods to scale multi-agent reinforcement learning algorithms to many agent scenarios using mean field theory. Previous work in this field assumes that an agent has access to exact cumulative metrics regarding the mean field behaviour of the system, which it can then use to take its actions. In this paper, we relax this assumption and maintain a distribution to model the uncertainty regarding the mean field of the system. We consider two different settings for this problem. In the first setting, only agents in a fixed neighbourhood are visible, while in the second setting, the visibility of agents is determined at random based on distances. For each of these settings, we introduce a Q-learning based algorithm that can learn effectively. We prove that this Q-learning estimate stays very close to the Nash Q-value (under a common set of assumptions) for the first setting. We also empirically show our algorithms outperform multiple baselines in three different games in the MAgents framework, which supports large environments with many agents learning simultaneously to achieve possibly distinct goals.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">MARLEmpircal</abbr>
    
  
  </div>

  <div id="lee2021neuripsdeeprl" class="col-sm-8">
      <div class="title">
          
          Investigation of Independent Reinforcement Learning Algorithms in Multi-Agent Environments
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://www.linkedin.com/in/km-lee/" target="_blank">Ken Ming Lee</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://sriramsubramanian.com/" target="_blank">Sriram Ganapathi Subramanian</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>NeurIPS 2021 Deep Reinforcement Learning Workshop</em>. 

      

      
      
      
          Dec,
      
      
        2021.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2111.01100" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         Independent reinforcement learning algorithms have no theoretical guarantees for finding the best policy in multi-agent settings. However, in practice, prior works have reported good performance with independent algorithms in some domains and bad performance in others. Moreover, a comprehensive study of the strengths and weaknesses of independent algorithms is lacking in the literature. In this paper, we carry out an empirical comparison of the performance of independent algorithms on four PettingZoo environments that span the three main categories of multi-agent environments, i.e., cooperative, competitive, and mixed. We show that in fully-observable environments, independent algorithms can perform on par with multi-agent algorithms in cooperative and competitive settings. For the mixed environments, we show that agents trained via independent algorithms learn to perform well individually, but fail to learn to cooperate with allies and compete with enemies. We also show that adding recurrence improves the learning of independent algorithms in cooperative partially observable environments.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">WildfireMLRev</abbr>
    
  
  </div>

  <div id="jain2020review" class="col-sm-8">
      <div class="title">
          
          A review of machine learning applications in wildfire science and management
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      Piyush Jain,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Sean CP Coogan,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://sriramsubramanian.com/" target="_blank">Sriram Ganapathi Subramanian</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    <em>Mark Crowley</em>,
                  
                
              
        
            
            
              
                
                  
                    
                      <a href="https://cfs.nrcan.gc.ca/employees/read/staylor" target="_blank">Steve Taylor</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      and Mike D Flannigan.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
        <em>Environmental Reviews</em>.

          
              28,
          
          
              (3).
          

      

      
          Canadian Science Publishing,
      
      
      
          Jul,
      
      
        2020.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2003.00646" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
      
      <a href="/assets/pdf/2020-envrevjrnl-jain-review.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
      
      <a href="https://www.dropbox.com/scl/fi/5236qksp44pp96tll8egl/2020-envrevjrnl-jain-review1.pdf?rlkey=1ycxvh8tyfxi8cxlzcctxch2y&raw=1" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
      <a href="https://cdnsciencepub.com/doi/10.1139/er-2020-0019" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         
Artificial intelligence has been applied in wildfire science and management since the 1990s, with early applications including neural networks and expert systems. Since then the field has rapidly progressed congruently with the wide adoption of machine learning (ML) methods in the environmental sciences. Here, we present a scoping review of ML applications in wildfire science and management. Our overall objective is to improve awareness of ML methods among wildfire researchers and managers, as well as illustrate the diverse and challenging range of problems in wildfire science available to ML data scientists. To that end, we first present an overview of popular ML approaches used in wildfire science to date, and then review the use of ML in wildfire science as broadly categorized into six problem domains, including: 1) fuels characterization, fire detection, and mapping; 2) fire weather and climate change; 3) fire occurrence, susceptibility, and risk; 4) fire behavior prediction; 5) fire effects; and 6) fire management. Furthermore, we discuss the advantages and limitations of various ML approaches relating to data size, computational requirements, generalizability, and interpretability, as well as identify opportunities for future advances in the science and management of wildfires within a data science context. In total, we identified 298 relevant publications, where the most frequently used ML methods across problem domains included random forests, MaxEnt, artificial neural networks, decision trees, support vector machines, and genetic algorithms. As such, there exists opportunities to apply more current ML methods — including deep learning and agent based learning — in the wildfire sciences, especially in instances involving very large multivariate datasets. We must recognize, however, that despite the ability of ML models to learn on their own, expertise in wildfire science is necessary to ensure realistic modelling of fire processes across multiple scales, while the complexity of some ML methods, such as deep learning, requires a dedicated and sophisticated knowledge of their application. Finally, we stress that the wildfire research and management communities play an active role in providing relevant, high quality, and freely available wildfire data for use by practitioners of ML methods.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="subramanian2018neurips-ai4sg" class="col-sm-8">
      <div class="title">
          
          A Complementary Approach to Improve WildFire Prediction Systems.
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://sriramsubramanian.com/" target="_blank">Sriram Ganapathi Subramanian</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>Neural Information Processing Systems (AI for social good workshop)</em>. 

      

      
      
          NeurIPS.
      
      
      
        2018.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
    
    
    
      
      <a href="/assets/pdf/2018-neurips-ai-subramanian-a%20complementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://aiforsocialgood.github.io/2018/acceptedpapers.htm" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">MCTS+A3C</abbr>
    
  
  </div>

  <div id="Subramanian2018CCAI" class="col-sm-8">
      <div class="title">
          
          Combining MCTS and A3C for prediction of spatially spreading processes in forest wildfire settings
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://sriramsubramanian.com/" target="_blank">Sriram Ganapathi Subramanian</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>Canadian Conference on Artificial Intelligence</em>. 

      

      
      
          Toronto, Ontario, Canada.
      
      
      
        2018.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2018-canai-ganapathi%20subramanian-combining.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://doi.org/10.1007/978-3-319-89656-4_28" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         In recent years, Deep Reinforcement Learning (RL) algorithms have shown super-human performance in a variety Atari and classic board games like chess and GO. Research into applications of RL in other domains with spatial considerations like environmental planning are still in their nascent stages. In this paper, we introduce a novel combination of Monte-Carlo Tree Search (MCTS) and A3C algorithms on an online simulator of a wildfire, on a pair of forest fires in Northern Alberta (Fort McMurray and Richardson fires) and on historical Saskatchewan fires previously compared by others to a physics-based simulator. We conduct several experiments to predict fire spread for several days before and after the given spatial information of fire spread and ignition points. Our results show that the advancements in Deep RL applications in the gaming world have advantages in spatially spreading real-world problems like forest fires. \textcopyright Springer International Publishing AG, part of Springer Nature 2018.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ganapathisubramanian2018frontict" class="col-sm-8">
      <div class="title">
          
          Using Spatial Reinforcement Learning to Build Forest Wildfire Dynamics Models From Satellite Images
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://sriramsubramanian.com/" target="_blank">Sriram Ganapathi Subramanian</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
        <em>Frontiers in ICT</em>.

          
              5,
          
          
              (6).
          

      

      
          Frontiers,
      
      
      
          Apr,
      
      
        2018.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2018-frontict-ganapathi%20subramanian-using" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="http://journal.frontiersin.org/article/10.3389/fict.2018.00006/full" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         Machine learning algorithms have increased tremendously in power in recent years but have yet to be fully utilized in many ecology and sustainable resource management domains such as wildlife reserve design, forest fire management and invasive species spread. One thing these domains have in common is that they contain dynamics that can be characterized as a Spatially Spreading Process (SSP) which requires many parameters to be set precisely to model the dynamics, spread rates and directional biases of the elements which are spreading. We present related work in Artificial Intelligence and Machine Learning for SSP sustainability domains including forest wildfire prediction. We then introduce a novel approach for learning in SSP domains using Reinforcement Learning (RL) where fire is the agent at any cell in the landscape and the set of actions the fire can take from a location at any point in time includes spreading North, South, East, West or not spreading. This approach inverts the usual RL setup since the dynamics of the corresponding Markov Decision Process (MDP) is a known function for immediate wildfire spread. Meanwhile, we learn an agent policy for a predictive model of the dynamics of a complex spatially-spreading process. Rewards are provided for correctly classifying which cells are on fire or not compared to satellite and other related data. We examine the behaviour of five RL algorithms on this problem: Value Iteration, Policy Iteration, Q-Learning, Monte Carlo Tree Search and Asynchronous Advantage Actor-Critic (A3C). We compare to a Gaussian process based supervised learning approach and discuss the relation of our approach to manually constructed, state-of-the-art methods from forest wildfire modelling. We also discuss the relation of our approach to manually constructed, state-of-the-art methods from forest wildfire modelling. We validate our approach with satellite image data of two massive wildfire events in Northern Alberta, Canada, the Fort McMurray fire of 2016 and the Richardson fire of 2011. The results show that we can learn predictive, agent-based policies as models of spatial dynamics using RL on readily available satellite images that other methods and have many additional advantages in terms of generalizability and interpretability.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="subramanian2017rldm" class="col-sm-8">
      <div class="title">
          
          Learning Forest Wildfire Dynamics from Satellite Images Using Reinforcement Learning
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      <a href="https://sriramsubramanian.com/" target="_blank">Sriram Ganapathi Subramanian</a>,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>Conference on Reinforcement Learning and Decision Making</em>. 

      

      
      
          Ann Arbor, MI, USA..
      
      
      
        2017.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="crowley2014ieeetoc" class="col-sm-8">
      <div class="title">
          
          Using equilibrium policy gradients for spatiotemporal planning in forest ecosystem management
      </div>
      <div class="author">
        
            
            
              
                
                   <em>Mark Crowley</em>.
                
              
        
      </div>

      <div class="periodical">
      
        <em>IEEE Transactions on Computers</em>.

          
              63,
          
          
              (1).
          

      

      
          IEEE computer Society Digital Library. IEEE Computer Society.,
      
      
      
      
        2014.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
      <a href="http://www.computer.org/csdl/trans/tc/preprint/06514032-abs.html" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         Spatiotemporal planning involves making choices at multiple locations in space over some planning horizon to maximize utility and satisfy various constraints. In Forest Ecosystem Management, the problem is to choose actions for thousands of locations each year including harvesting, treating trees for fire or pests, or doing nothing. The utility models could place value on sale of lumber, ecosystem sustainability or employment levels and incorporate legal and logistical constraints on actions such as avoiding large contiguous areas of clearcutting. Simulators developed by forestry researchers provide detailed dynamics but are generally inaccesible black boxes. We model spatiotemporal planning as a factored Markov decision process and present a policy gradient planning algorithm to optimize a stochastic spatial policy using simulated dynamics. It is common in environmental and resource planning to have actions at different locations be spatially interelated, this makes representation and planning challenging. We define a global spatial policy in terms of interacting local policies defining distributions over actions at each location conditioned on actions at nearby locations. Markov chain Monte Carlo simulation is used to sample landscape policies and estimate their gradients. Evaluation is carried out on a forestry planning problem with 1,880 locations using a variety of value models and constraints. Index</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="houtman2013ijwf" class="col-sm-8">
      <div class="title">
          
          Allowing a wildfire to burn: Estimating the effect on future fire suppression costs
      </div>
      <div class="author">
        
            
            
              
                
                  
                    
                      Rachel M. Houtman,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Claire A. Montgomery,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Aaron R. Gagnon,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      David E. Calkin,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Thomas G. Dietterich,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      Sean McGregor,
                    
                  
                
              
        
            
            
              
                
                  
                    and <em>Mark Crowley</em>
                  
                
              
        
      </div>

      <div class="periodical">
      
        <em>International Journal of Wildland Fire</em>.

          
              22,
          
          
              (7).
          

      

      
      
      
      
        2013.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2013-ijwf-houtman-allowing.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://www.publish.csiro.au/wf/WF12157" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         Where a legacy of aggressive wildland fire suppression has left forests in need of fuel reduction, allowing wildland fire to burn may provide fuel treatment benefits, thereby reducing suppression costs from subsequent fires. The least-cost-plus-net-value-change model of wildland fire economics includes benefits of wildfire in a framework for evaluating suppression options. In this study, we estimated one component of that benefit – the expected present value of the reduction in suppression costs for subsequent fires arising from the fuel treatment effect of a current fire. To that end, we employed Monte Carlo methods to generate a set of scenarios for subsequent fire ignition and weather events, which are referred to as sample paths, for a study area in central Oregon. We simulated fire on the landscape over a 100-year time horizon using existing models of fire behaviour, vegetation and fuels development, and suppression effectiveness, and we estimated suppression costs using an existing suppression cost model. Our estimates suggest that the potential cost savings may be substantial. Further research is needed to estimate the full least-cost-plus-net-value-change model. This line of research will extend the set of tools available for developing wildfire management plans for forested landscapes.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">phd-thesis</abbr>
    
  
  </div>

  <div id="Crowley2011thesis" class="col-sm-8">
      <div class="title">
          
          Equilibrium Policy Gradients for Spatiotemporal Planning
      </div>
      <div class="author">
        
            
            
              
                
                   <em>Mark Crowley</em>.
                
              
        
      </div>

      <div class="periodical">
      

      
          UBC Library,
      
      
          Vancouver, BC, Canada..
      
      
      
        2011.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2011-ubclibrary-crowley-equilibrium.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://open.library.ubc.ca/collections/ubctheses/24/items/1.0052093" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         In spatiotemporal planning, agents choose actions at multiple locations in space over some planning horizon to maximize their utility and satisfy various constraints. In forestry planning, for example, the problem is to choose actions for thousands of locations in the forest each year. The actions at each location could include harvesting trees, treating trees against disease and pests, or doing nothing. A utility model could place value on sale of forest products, ecosystem sustainability or employment levels, and could incorporate legal and logistical constraints such as avoiding large contiguous areas of clearcutting and managing road access. Planning requires a model of the dynamics. Existing simulators developed by forestry researchers can provide detailed models of the dynamics of a forest over time, but these simulators are often not designed for use in automated planning. This thesis presents spatiotemoral planning in terms of factored Markov decision processes. A policy gradient planning algorithm optimizes a stochastic spatial policy using existing simulators for dynamics. When a planning problem includes spatial interaction between locations, deciding on an action to carry out at one location requires considering the actions performed at other locations. This spatial interdependence is common in forestry and other environmental planning problems and makes policy representation and planning challenging. We define a spatial policy in terms of local policies defined as distributions over actions at one location conditioned upon actions at other locations. A policy gradient planning algorithm using this spatial policy is presented which uses Markov Chain Monte Carlo simulation to sample the landscape policy, estimate its gradient and use this gradient to guide policy improvement. Evaluation is carried out on a forestry planning problem with 1880 locations using a variety of value models and constraints. The distribution over joint actions at all locations can be seen as the equilibrium of a cyclic causal model. This equilibrium semantics is compared to Structural Equation Models. We also define an algorithm for approximating the equilibrium distribution for cyclic causal networks which exploits graphical structure and analyse when the algorithm is exact.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="Crowley2009" class="col-sm-8">
      <div class="title">
          
          Seeing the Forest Despite the Trees : Large Scale Spatial-Temporal Decision Making
      </div>
      <div class="author">
        
            
            
              
                
                  
                    <em>Mark Crowley</em>,
                  
                
              
        
            
            
              
                
                  
                    
                      John Nelson,
                    
                  
                
              
        
            
            
              
                
                  
                    
                      and <a href="https://www.cs.ubc.ca/~poole/" target="_blank">David Poole</a>.
                    
                  
                
              
        
      </div>

      <div class="periodical">
      
          In <em>Conference on Uncertainty in Artificial Intelligence (UAI09)</em>. 

      

      
      
          Montreal, Canada.
      
      
      
        2009.
      

       
      
        
      <div class="periodical">
          
      </div>
      </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
      <a href="http://www.cs.ubc.ca/ crowley/papers/uai09-mark-crowley.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    

    
    </div>

    <!-- Hidden notes block - pick note or annote and stick to it -->
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
        <p>
         We introduce a challenging real world planning problem where actions must be taken at each location in a spatial area at each point in time. We use forestry planning as the motivating application. In Large Scale Spatial-Temporal (LSST) planning problems, the state and action spaces are defined as the cross-products of many local state and action spaces spread over a large spatial area such as a city or forest. These problems possess state uncertainty, have complex utility functions involving spatial constraints and we generally must rely on simulations rather than an explicit transition model. We define LSST problems as reinforcement learning prob- lems and present a solution using policy gradients. We compare two different policy formulations: an explicit policy that identifies each location in space and the action to take there, and an abstract policy that defines the proportion of actions to take across all locations in space. We show that the abstract policy is more robust and achieves higher rewards with far fewer parameters than the elementary policy. This abstract policy is also a better fit to the properties that practitioners in LSST problem domains require for such methods to be widely useful</p>
    </div>
    
  </div>
</div>
</li></ol>
            </div>
          
          
      

    
</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Mark  Crowley.
    Based on [*folio](https://github.com/bogoli/-folio) design. Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: April 09, 2025.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  



<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
